# -*- coding: utf-8 -*-
"""streamlitapp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10LhZIz1Py09kqFEBauTQHMkQt_fk5ifl

## Chatbot NLP pour le Système Hydraulique

Cette section implémente un assistant conversationnel pour interagir avec les modèles prédictifs et l'utilisateur.
"""

!pip install streamlit==1.38.0 pandas==2.2.3 numpy==2.0.2 scikit-learn==1.5.2 tensorflow==2.18.0 keras-tcn==3.5.6 openpyxl==3.1.2 reportlab==4.2.2 matplotlib==3.9.2 seaborn==0.13.2 nltk==3.9.1 joblib==1.4.2 plotly==5.24.1

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import shutil

DRIVE_BASE_PATH = '/content/drive/MyDrive/Projet_time_series'
DRIVE_MODEL_PATH = os.path.join(DRIVE_BASE_PATH, 'Models')
DRIVE_DATA_PATH = os.path.join(DRIVE_BASE_PATH, 'Data_processed')
DRIVE_HISTORY_PATH = os.path.join(DRIVE_BASE_PATH, 'History')
DRIVE_STREAMLIT_PATH = '/content/streamlit_app'

# Créer les dossiers sur Google Drive
for path in [DRIVE_MODEL_PATH, DRIVE_DATA_PATH, DRIVE_HISTORY_PATH]:
    os.makedirs(path, exist_ok=True)

# Créer le dossier pour Streamlit
if os.path.exists(DRIVE_STREAMLIT_PATH):
    shutil.rmtree(DRIVE_STREAMLIT_PATH)
os.makedirs(DRIVE_STREAMLIT_PATH)

# Vérifier si merged_resampled_1hz_no_t0.csv existe
data_file = os.path.join(DRIVE_DATA_PATH, 'merged_resampled_1hz_no_t0.csv')
if not os.path.exists(data_file):
    print(f"Erreur : {data_file} non trouvé. Veuillez le télécharger sur Google Drive.")
else:
    print(f"Fichier de données trouvé : {data_file}")

    # Charger les données
    df = pd.read_csv(data_file)

    # Vérifier les colonnes nécessaires
    required_columns = ['cycle_id', 'Temps (s)', 'PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'CE', 'SE']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"Colonnes manquantes dans {data_file} : {missing_columns}")
    else:
        print("Toutes les colonnes nécessaires sont présentes.")


        static_data = df.groupby('cycle_id')[['PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'CE', 'SE']].mean().reset_index()
        static_data_path = os.path.join(DRIVE_DATA_PATH, 'static_data.csv')
        static_data.to_csv(static_data_path, index=False)
        print(f"Données statiques générées et sauvegardées : {static_data_path}")

import pandas as pd
import os

# Define paths
DRIVE_DATA_PATH = '/content/drive/MyDrive/Projet_time_series/Data_processed'
SAMPLE_DATA_FILE = 'sample_data.csv'
NOTEBOOK_DATA_FILE = 'merged_resampled_1hz_no_t0.csv'

# Create sample data
sample_data = pd.DataFrame({
    'cycle_id': [0] * 15 + [1] * 15,  # Two cycles for sufficient sequence length
    'Temps (s)': list(range(1, 16)) * 2,
    'PS1': [184.7723, 189.7699, 191.3452, 191.2842, 191.3105] * 6,
    'PS2': [0.19044, 0.02297, 0.00283, 0.0, 0.0] * 6,
    'PS3': [0.0] * 30,
    'PS5': [9.95992, 9.96455, 9.96817, 9.97272, 9.97742] * 6,
    'PS6': [9.83701, 9.83842, 9.84580, 9.84868, 9.85498] * 6,
    'EPS1': [2920.048, 2858.938, 2946.510, 2946.164, 2945.786] * 6,
    'FS1': [0.0008, 0.0020, 0.0011, 0.0010, 0.0017] * 6,
    'FS2': [10.1706, 10.1637, 10.1747, 10.1758, 10.1731] * 6,
    'TS1': [35.867] * 30,
    'TS2': [40.558] * 30,
    'TS3': [38.671] * 30,
    'TS4': [30.363] * 30,
    'VS1': [0.604, 0.605, 0.611, 0.603, 0.608] * 6,
    'CE': [47.202, 47.273, 47.250, 47.332, 47.213] * 6,
    'CP': [2.184, 2.184, 2.184, 2.185, 2.178] * 6,
    'SE': [68.039, 0.0, 0.0, 0.0, 0.0] * 6,
    'Cooler_condition': [100] * 30,
    'Valve_condition': [100] * 30,
    'Internal_pump_leakage': [0] * 30,
    'Hydraulic_accumulator': [130] * 30,
    'Stable_flag': [1] * 30
})

# Save sample data to Google Drive
sample_data_path = os.path.join(DRIVE_DATA_PATH, SAMPLE_DATA_FILE)
sample_data.to_csv(sample_data_path, index=False)
print(f"Created and saved {SAMPLE_DATA_FILE} to {DRIVE_DATA_PATH}")

# Check if notebook data exists, otherwise use sample data as placeholder
notebook_data_path = os.path.join(DRIVE_DATA_PATH, NOTEBOOK_DATA_FILE)
if not os.path.exists(notebook_data_path):
    sample_data.to_csv(notebook_data_path, index=False)
    print(f"Created placeholder {NOTEBOOK_DATA_FILE} using sample data at {DRIVE_DATA_PATH}")

import os
import pandas as pd
import shutil

DRIVE_BASE_PATH = '/content/drive/MyDrive/Projet_time_series'
DRIVE_MODEL_PATH = os.path.join(DRIVE_BASE_PATH, 'Models')
DRIVE_DATA_PATH = os.path.join(DRIVE_BASE_PATH, 'Data_processed')
DRIVE_HISTORY_PATH = os.path.join(DRIVE_BASE_PATH, 'History')
DRIVE_STREAMLIT_PATH = '/content/streamlit_app'

# Créer les dossiers sur Google Drive
for path in [DRIVE_MODEL_PATH, DRIVE_DATA_PATH, DRIVE_HISTORY_PATH]:
    os.makedirs(path, exist_ok=True)

# Créer le dossier pour Streamlit et le sous-répertoire pages
if os.path.exists(DRIVE_STREAMLIT_PATH):
    shutil.rmtree(DRIVE_STREAMLIT_PATH)
os.makedirs(DRIVE_STREAMLIT_PATH, exist_ok=True)
os.makedirs(os.path.join(DRIVE_STREAMLIT_PATH, 'pages'), exist_ok=True)  # Ajout de la création du répertoire pages/

# Vérifier si merged_resampled_1hz_no_t0.csv existe
data_file = os.path.join(DRIVE_DATA_PATH, 'merged_resampled_1hz_no_t0.csv')
if not os.path.exists(data_file):
    print(f"Erreur : {data_file} non trouvé. Veuillez le télécharger sur Google Drive.")
else:
    print(f"Fichier de données trouvé : {data_file}")

    # Charger les données
    df = pd.read_csv(data_file)

    # Vérifier les colonnes nécessaires
    required_columns = ['cycle_id', 'Temps (s)', 'PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'CE', 'SE']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"Colonnes manquantes dans {data_file} : {missing_columns}")
    else:
        print("Toutes les colonnes nécessaires sont présentes.")

        static_data = df.groupby('cycle_id')[['PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'CE', 'SE']].mean().reset_index()
        static_data_path = os.path.join(DRIVE_DATA_PATH, 'static_data.csv')
        static_data.to_csv(static_data_path, index=False)
        print(f"Données statiques générées et sauvegardées : {static_data_path}")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/utils.py
# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import MinMaxScaler, StandardScaler
# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# import tensorflow as tf
# from tensorflow.keras.models import load_model
# from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Conv1D, Input
# from tensorflow.keras.optimizers import Adam
# import os
# import matplotlib.pyplot as plt
# import seaborn as sns
# from reportlab.lib.pagesizes import letter
# from reportlab.pdfgen import canvas
# import nltk
# from nltk.tokenize import word_tokenize
# from nltk.corpus import stopwords
# import re
# from joblib import load
# 
# # Download NLTK resources
# nltk.download('punkt', quiet=True)
# nltk.download('punkt_tab', quiet=True)
# nltk.download('stopwords', quiet=True)
# 
# # Chemins Google Drive
# DRIVE_BASE_PATH = '/content/drive/MyDrive/Projet_time_series'
# DRIVE_MODEL_PATH = os.path.join(DRIVE_BASE_PATH, 'Models')
# DRIVE_DATA_PATH = os.path.join(DRIVE_BASE_PATH, 'Data_processed')
# DRIVE_HISTORY_PATH = os.path.join(DRIVE_BASE_PATH, 'History')
# DYNAMIC_DATA_FILE = 'merged_resampled_1hz_no_t0.csv'
# STATIC_DATA_FILE = 'static_data.csv'
# NOTEBOOK_DATA_FILE = 'merged_resampled_1hz_no_t0.csv'
# 
# # Créer les dossiers sur Google Drive si nécessaire
# for path in [DRIVE_MODEL_PATH, DRIVE_DATA_PATH, DRIVE_HISTORY_PATH]:
#     os.makedirs(path, exist_ok=True)
# 
# # Capteurs
# SENSOR_COLUMNS = ['PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'TS2', 'TS3', 'TS4', 'VS1', 'CE', 'CP', 'SE']
# FEATURE_COLS = SENSOR_COLUMNS[:-1]
# TARGET_COL = 'SE'
# STATIC_SENSOR_COLUMNS = ['PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'CE']
# 
# # Création de séquences pour modèles dynamiques
# def create_sequences(df, seq_length, target_col, feature_cols):
#     X, y = [], []
#     scaler = MinMaxScaler()
#     df_scaled = df.copy()
#     df_scaled[feature_cols] = scaler.fit_transform(df[feature_cols])
#     df_scaled[target_col] = scaler.fit_transform(df[[target_col]])
# 
#     for cycle_id in df['cycle_id'].unique():
#         df_cycle = df_scaled[df_scaled['cycle_id'] == cycle_id]
#         data = df_cycle[feature_cols + [target_col]].values
#         for i in range(len(data) - seq_length):
#             X.append(data[i:i + seq_length, :-1])
#             y.append(data[i + seq_length, -1])
#     return np.array(X), np.array(y), scaler
# 
# # Préparer les données statiques
# def prepare_static_data():
#     static_data_path = os.path.join(DRIVE_DATA_PATH, STATIC_DATA_FILE)
#     if not os.path.exists(static_data_path):
#         raise FileNotFoundError(f"Fichier {static_data_path} non trouvé. Exécutez la cellule de création de données.")
#     data = pd.read_csv(static_data_path)
#     return data
# 
# # Modèle LSTM
# def build_lstm_model(input_shape):
#     inputs = Input(shape=input_shape)
#     x = LSTM(64, return_sequences=True)(inputs)
#     x = LSTM(32)(x)
#     x = Dropout(0.2)(x)
#     x = Dense(16, activation='relu')(x)
#     outputs = Dense(1)(x)
#     model = tf.keras.Model(inputs, outputs)
#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
#     return model
# 
# # Modèle GRU
# def build_gru_model(input_shape):
#     inputs = Input(shape=input_shape)
#     x = GRU(64, return_sequences=True)(inputs)
#     x = GRU(32)(x)
#     x = Dropout(0.2)(x)
#     x = Dense(16, activation='relu')(x)
#     outputs = Dense(1)(x)
#     model = tf.keras.Model(inputs, outputs)
#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
#     return model
# 
# # Modèle CNN-LSTM
# def build_cnn_lstm_model(input_shape):
#     inputs = Input(shape=input_shape)
#     x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)
#     x = LSTM(64, return_sequences=True)(x)
#     x = LSTM(32)(x)
#     x = Dropout(0.2)(x)
#     x = Dense(16, activation='relu')(x)
#     outputs = Dense(1)(x)
#     model = tf.keras.Model(inputs, outputs)
#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
#     return model
# 
# # Charger les modèles et scalers
# def load_models():
#     models = {}
#     scalers = {}
# 
#     # Modèles dynamiques (LSTM, GRU, CNN-LSTM)
#     for model_name in ['lstm', 'gru', 'cnn-lstm']:
#         model_path_h5 = os.path.join(DRIVE_MODEL_PATH, f'direct_{model_name.replace("_", "-")}_se_pred_v1.h5')
#         model_path_keras = os.path.join(DRIVE_MODEL_PATH, f'direct_{model_name}_se_pred_v1.keras')
# 
#         try:
#             if os.path.exists(model_path_keras):
#                 models[model_name] = load_model(model_path_keras)
#                 print(f"Chargé {model_path_keras}")
#             elif os.path.exists(model_path_h5):
#                 model = load_model(model_path_h5, compile=False)
#                 model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
#                 model.save(model_path_keras)
#                 models[model_name] = model
#                 print(f"Chargé {model_path_h5} et converti en {model_path_keras}")
#             else:
#                 print(f"Aucun fichier trouvé pour {model_name} : {model_path_h5} ou {model_path_keras}")
#         except Exception as e:
#             print(f"Erreur lors du chargement de {model_name} : {e}")
# 
#     # Modèles statiques (LSTM_SE, GRU_SE, MLP_INVERSE)
#     try:
#         lstm_se_path = os.path.join(DRIVE_MODEL_PATH, 'model_lstm_se.h5')
#         if os.path.exists(lstm_se_path):
#             models['lstm_se'] = load_model(lstm_se_path)
#             print(f"Chargé {lstm_se_path}")
#         else:
#             print(f"Aucun fichier trouvé pour lstm_se : {lstm_se_path}")
#     except Exception as e:
#         print(f"Erreur lors du chargement de lstm_se : {e}")
# 
#     try:
#         gru_se_path = os.path.join(DRIVE_MODEL_PATH, 'model_gru_se.h5')
#         if os.path.exists(gru_se_path):
#             models['gru_se'] = load_model(gru_se_path)
#             print(f"Chargé {gru_se_path}")
#         else:
#             print(f"Aucun fichier trouvé pour gru_se : {gru_se_path}")
#     except Exception as e:
#         print(f"Erreur lors du chargement de gru_se : {e}")
# 
#     try:
#         mlp_inverse_path = os.path.join(DRIVE_MODEL_PATH, 'model_inverse_mlp.joblib')
#         if os.path.exists(mlp_inverse_path):
#             models['mlp_inverse'] = load(mlp_inverse_path)
#             print(f"Chargé {mlp_inverse_path}")
#         else:
#             print(f"Aucun fichier trouvé pour mlp_inverse : {mlp_inverse_path}")
#     except Exception as e:
#         print(f"Erreur lors du chargement de mlp_inverse : {e}")
# 
#     # Scalers
#     scaler_paths = {
#         'scaler_X_se': 'scaler_X_se.joblib',
#         'scaler_y_se': 'scaler_y_se.joblib',
#         'scaler_X_inverse': 'scaler_X_inverse.joblib',
#         'scaler_y_inverse': 'scaler_y_inverse.joblib',
#         'scaler_dynamic': 'scaler_dynamic.joblib'
#     }
#     for scaler_name, file_name in scaler_paths.items():
#         scaler_path = os.path.join(DRIVE_MODEL_PATH, file_name)
#         try:
#             if os.path.exists(scaler_path):
#                 scalers[scaler_name] = load(scaler_path)
#                 print(f"Chargé {scaler_path}")
#             else:
#                 print(f"Aucun fichier trouvé pour {scaler_name} : {scaler_path}")
#         except Exception as e:
#             print(f"Erreur lors du chargement de {scaler_name} : {e}")
# 
#     return models, scalers
# 
# # Sauvegarder les prédictions
# def save_predictions(predictions, filename):
#     df = pd.DataFrame(predictions, columns=['Predicted_SE'])
#     save_path = os.path.join(DRIVE_HISTORY_PATH, filename)
#     df.to_csv(save_path, index=False)
#     return df
# 
# # Générer un rapport PDF
# def generate_pdf_report(predictions, metrics, filename):
#     save_path = os.path.join(DRIVE_HISTORY_PATH, filename)
#     c = canvas.Canvas(save_path, pagesize=letter)
#     c.drawString(100, 750, "Rapport de Prédiction")
#     c.drawString(100, 730, f"Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
#     c.drawString(100, 710, f"Métriques: MSE={metrics['MSE']:.4f}, MAE={metrics['MAE']:.4f}, R²={metrics['R²']:.4f}")
#     c.drawString(100, 690, "Premières prédictions:")
#     for i, pred in enumerate(predictions[:5]):
#         c.drawString(120, 670 - i * 20, f"Prédiction {i+1}: {pred:.4f}")
#     c.save()
# 
# # Plotter les séries temporelles
# def plot_time_series(df, sensors, cycle_id=None):
#     if cycle_id is not None:
#         df = df[df['cycle_id'] == cycle_id]
#     fig, ax = plt.subplots(figsize=(12, 6))
#     for sensor in sensors:
#         ax.plot(df['Temps (s)'], df[sensor], label=sensor)
#     ax.set_title(f"Séries temporelles (Cycle {cycle_id if cycle_id is not None else 'Tous'})")
#     ax.set_xlabel("Temps (s)")
#     ax.set_ylabel("Valeur")
#     ax.legend()
#     ax.grid(True)
#     return fig
# 
# # Plotter la heatmap de corrélation
# def plot_correlation_heatmap(df, columns):
#     fig, ax = plt.subplots(figsize=(10, 8))
#     corr = df[columns].corr()
#     sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', ax=ax)
#     ax.set_title("Matrice de corrélation")
#     return fig
# 
# # Plotter les prédictions vs vraies valeurs
# def plot_predictions(y_true, y_pred, title="Prédictions vs Vraies Valeurs"):
#     fig, ax = plt.subplots(figsize=(12, 6))
#     ax.plot(y_true, label='Vrai SE', color='blue')
#     ax.plot(y_pred, label='Prédit SE', color='orange', linestyle='--')
#     ax.set_title(title)
#     ax.set_xlabel("Échantillon")
#     ax.set_ylabel("SE")
#     ax.legend()
#     ax.grid(True)
#     return fig
# 
# # Plotter la comparaison des prédictions des modèles
# def plot_model_predictions(y_true, predictions_dict, title="Comparaison des Prédictions des Modèles"):
#     fig, ax = plt.subplots(figsize=(12, 6))
#     ax.plot(y_true, label='Vrai SE', color='blue')
#     for model_name, y_pred in predictions_dict.items():
#         ax.plot(y_pred, label=f'{model_name.upper()}', linestyle='--')
#     ax.set_title(title)
#     ax.set_xlabel("Échantillon")
#     ax.set_ylabel("SE")
#     ax.legend()
#     ax.grid(True)
#     return fig
# 
# # Générer un graphique de comparaison des métriques
# def plot_comparison(metrics_dict):
#     df = pd.DataFrame(metrics_dict).T
#     fig, ax = plt.subplots(figsize=(10, 6))
#     sns.barplot(data=df, ax=ax)
#     ax.set_title("Comparaison des métriques des modèles")
#     ax.set_ylabel("Valeur")
#     plt.xticks(rotation=45)
#     return fig
# 
# # Classe HydraulicNLPAssistant
# class HydraulicNLPAssistant:
#     def __init__(self, models, scalers):
#         self.model_lstm = models.get('lstm_se')
#         self.model_gru = models.get('gru_se')
#         self.model_mlp = models.get('mlp_inverse')
#         self.scaler_X_se = scalers.get('scaler_X_se')
#         self.scaler_y_se = scalers.get('scaler_y_se')
#         self.scaler_X_mlp = scalers.get('scaler_X_inverse')
#         self.scaler_y_mlp = scalers.get('scaler_y_inverse')
#         self.stop_words = set(stopwords.words('french'))
#         self.intents = {
#             'predict_se': ['prédire', 'prévoir', 'efficacité', 'se'],
#             'optimize_se': ['optimiser', 'meilleur', 'paramètres', 'atteindre'],
#             'general': ['bonjour', 'aide', 'info']
#         }
#         self.sensor_columns = STATIC_SENSOR_COLUMNS
# 
#     def preprocess_text(self, text):
#         text = text.lower()
#         tokens = word_tokenize(text)
#         tokens = [t for t in tokens if t not in self.stop_words and (t.isalnum() or t.isdigit())]
#         return tokens
# 
#     def detect_intent(self, tokens):
#         for token in tokens:
#             for intent, keywords in self.intents.items():
#                 if token in keywords:
#                     return intent
#         return 'general'
# 
#     def extract_parameters(self, text):
#         numbers = re.findall(r'[-+]?[0-9]*\.?[0-9]+', text)
#         return [float(num) for num in numbers]
# 
#     def validate_sensors(self, sensor_values):
#         warnings = []
#         for col, val in zip(self.sensor_columns, sensor_values):
#             if col.startswith('PS') and val < 0:
#                 warnings.append(f"Avertissement: {col} pression ({val:.2f}) est négative.")
#             elif col.startswith('TS') and (val < 0 or val > 100):
#                 warnings.append(f"Avertissement: {col} température ({val:.2f}) hors plage 0-100°C.")
#             elif col == 'CE' and (val < 0 or val > 100):
#                 warnings.append(f"Avertissement: {col} efficacité ({val:.2f}) hors plage 0-100%.")
#         return warnings
# 
#     def predict_se(self, sensor_values, model):
#         try:
#             if len(sensor_values) != len(self.sensor_columns):
#                 return None, [f"Erreur: {len(sensor_values)} valeurs fournies, {len(self.sensor_columns)} attendues ({', '.join(self.sensor_columns)})."]
#             sensor_df = pd.DataFrame([sensor_values], columns=self.sensor_columns)
#             sensor_scaled = self.scaler_X_se.transform(sensor_df)
#             sensor_reshaped = sensor_scaled.reshape((sensor_scaled.shape[0], 1, sensor_scaled.shape[1]))
#             se_scaled = model.predict(sensor_reshaped, verbose=0)
#             se_original = self.scaler_y_se.inverse_transform(se_scaled)
#             se_clipped = np.clip(se_original[0][0], 0, 100)
#             return se_clipped, []
#         except Exception as e:
#             return None, [f"Erreur de prédiction SE: {e}"]
# 
#     def predict_sensor_values(self, target_se):
#         try:
#             if self.model_mlp is None:
#                 return np.zeros(len(self.sensor_columns)), ["Modèle MLP non chargé"]
#             se_scaled = self.scaler_X_mlp.transform(np.array([[target_se]]))
#             sensors_scaled = self.model_mlp.predict(se_scaled)
#             sensors_original = self.scaler_y_mlp.inverse_transform(sensors_scaled)
#             warnings = self.validate_sensors(sensors_original[0])
#             return sensors_original[0], warnings
#         except Exception as e:
#             return np.zeros(len(self.sensor_columns)), [f"Erreur de prédiction inverse: {str(e)}"]
# 
#     def process_message(self, message):
#         tokens = self.preprocess_text(message)
#         intent = self.detect_intent(tokens)
#         params = self.extract_parameters(message)
# 
#         if intent == 'predict_se':
#             if len(params) == len(self.sensor_columns):
#                 sensor_values = params
#                 se_lstm, warnings_lstm = self.predict_se(sensor_values, self.model_lstm) if self.model_lstm else (None, ["Modèle LSTM non chargé"])
#                 se_gru, warnings_gru = self.predict_se(sensor_values, self.model_gru) if self.model_gru else (None, ["Modèle GRU non chargé"])
#                 if se_lstm is not None or se_gru is not None:
#                     message = "Efficacité système (SE) prédite:\n"
#                     if se_lstm is not None:
#                         message += f"- LSTM: {se_lstm:.2f}%\n"
#                     if se_gru is not None:
#                         message += f"- GRU: {se_gru:.2f}%"
#                     warnings = warnings_lstm + warnings_gru
#                     return {
#                         'type': 'prediction',
#                         'message': message,
#                         'warnings': warnings
#                     }
#                 else:
#                     return {
#                         'type': 'error',
#                         'message': 'Aucun modèle chargé pour la prédiction.'
#                     }
#             else:
#                 return {
#                     'type': 'error',
#                     'message': f"Veuillez fournir {len(self.sensor_columns)} valeurs de capteurs ({', '.join(self.sensor_columns)})."
#                 }
#         elif intent == 'optimize_se':
#             if params:
#                 target_se = params[0]
#                 if 0 <= target_se <= 100:
#                     sensor_values, warnings = self.predict_sensor_values(target_se)
#                     sensor_dict = {col: round(val, 2) for col, val in zip(self.sensor_columns, sensor_values)}
#                     return {
#                         'type': 'optimization',
#                         'message': f"Pour atteindre SE de {target_se}%, définissez les valeurs suivantes:",
#                         'sensor_values': sensor_dict,
#                         'warnings': warnings
#                     }
#                 else:
#                     return {
#                         'type': 'error',
#                         'message': 'SE cible doit être entre 0 et 100.'
#                     }
#             else:
#                 return {
#                     'type': 'error',
#                     'message': 'Veuillez fournir une valeur SE cible (ex: "Optimiser pour SE 95").'
#                 }
#         else:
#             return {
#                 'type': 'general',
#                 'message': 'Bonjour! Je peux optimiser les paramètres pour une efficacité cible (SE) ou prédire SE à partir des capteurs. Ex: "Optimiser pour SE 95" ou "Prédire SE avec 95.13, 60.52, 0, 0, 0.25, 572.32, 7.34, 3.1, 40, 20".'
#             }

import sys
import os

sys.path.append('/content/streamlit_app')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/app.py
# 
# import streamlit as st
# 
# st.set_page_config(page_title="Prédiction de Séries Temporelles", layout="wide")
# 
# st.sidebar.title("Navigation")
# st.sidebar.markdown("""
# - [Tableau de Bord](Tableau_de_Bord)
# - [Chatbot](Chatbot)
# - [Accueil](Home)
# - [Données](Data)
# - [Prédiction](Prediction)
# - [Prédiction SE](Prediction_SE)
# - [Prédiction Inverse](Prediction_Inverse)
# - [Réentraînement](Retrain)
# - [Historique](History)
# - [Comparateur](Compare)
# """)
# st.sidebar.write("Dernière mise à jour: 02:17 PM +01, 15/06/2025")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile    /content/streamlit_app/pages/1_home.py
# 
# import streamlit as st
# import io
# 
# def main():
#     st.title("Accueil")
#     st.header("Prédiction de Séries Temporelles")
#     st.write("""
#     Cette application permet de prédire la variable `SE` (System Efficiency) à partir de données de capteurs industriels.
#     Fonctionnalités principales :
#     - **Exploration des données** : Visualiser les données du notebook (`merged_resampled_1hz_no_t0.csv`) ou un exemple.
#     - **Prédiction** : Utiliser les données du notebook ou uploader vos propres données pour prédire avec des modèles pré-entraînés (LSTM, GRU, CNN-LSTM).
#     - **Réentraînement** : Réentraîner les modèles avec de nouvelles données.
#     - **Historique** : Consulter les prédictions passées et leurs métriques.
#     - **Comparateur** : Comparer les performances des modèles.
#     """)
# 
#     st.subheader("Technologies utilisées")
#     st.write("- Python, Streamlit, TensorFlow, Pandas, Scikit-learn")
# 
#     st.subheader("Modèles utilisés")
#     st.write("""
#     Les modèles suivants sont utilisés pour prédire `SE` directement à partir des capteurs :
#     - **LSTM (Long Short-Term Memory)** : Réseau récurrent adapté aux séries temporelles, capturant les dépendances à long terme. Structure : deux couches LSTM (64 et 32 unités) suivies de couches denses.
#     - **GRU (Gated Recurrent Unit)** : Variante simplifiée du LSTM, plus rapide à entraîner. Structure similaire au LSTM.
#     - **CNN-LSTM** : Combine une couche convolutive (64 filtres) pour extraire des motifs locaux et des couches LSTM pour les dépendances temporelles.
# 
#     **Exemple de structure (LSTM)** :
#     ```python
#     from tensorflow.keras.layers import LSTM, Dense, Dropout
#     from tensorflow.keras.optimizers import Adam
#     from tensorflow.keras import Input, Model
# 
#     def build_lstm_model(input_shape):
#         inputs = Input(shape=input_shape)
#         x = LSTM(64, return_sequences=True)(inputs)
#         x = LSTM(32)(x)
#         x = Dropout(0.2)(x)
#         x = Dense(16, activation='relu')(x)
#         outputs = Dense(1)(x)
#         model = Model(inputs, outputs)
#         model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
#         return model
#     ```
#     """)
# 
#     # Code à télécharger
#     model_code = """
# from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Conv1D, Input
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras import Model
# 
# def build_lstm_model(input_shape):
#     inputs = Input(shape=input_shape)
#     x = LSTM(64, return_sequences=True)(inputs)
#     x = LSTM(32)(x)
#     x = Dropout(0.2)(x)
#     x = Dense(16, activation='relu')(x)
#     outputs = Dense(1)(x)
#     model = Model(inputs, outputs)
#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
#     return model
# 
# def build_gru_model(input_shape):
#     inputs = Input(shape=input_shape)
#     x = GRU(64, return_sequences=True)(inputs)
#     x = GRU(32)(x)
#     x = Dropout(0.2)(x)
#     x = Dense(16, activation='relu')(x)
#     outputs = Dense(1)(x)
#     model = Model(inputs, outputs)
#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
#     return model
# 
# def build_cnn_lstm_model(input_shape):
#     inputs = Input(shape=input_shape)
#     x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)
#     x = LSTM(64, return_sequences=True)(x)
#     x = LSTM(32)(x)
#     x = Dropout(0.2)(x)
#     x = Dense(16, activation='relu')(x)
#     outputs = Dense(1)(x)
#     model = Model(inputs, outputs)
#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
#     return model
# """
# 
#     st.download_button(
#         label="Télécharger les structures des modèles (Python)",
#         data=model_code.encode('utf-8'),
#         file_name="model_structures.py",
#         mime="text/python"
#     )
# 
#     st.subheader("Commencer")
#     st.write("Naviguez via la barre latérale pour explorer les fonctionnalités.")
# 
# if __name__ == "__main__":
#     main()
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/pages/2_data.py
# import streamlit as st
# import pandas as pd
# import os
# from utils import SENSOR_COLUMNS, DRIVE_DATA_PATH, NOTEBOOK_DATA_FILE, plot_correlation_heatmap, plot_time_series
# 
# def main():
#     st.title("Exploration des Données")
#     st.header("Analyser les données des capteurs")
# 
#     data_source = st.radio("Source des données", ["Uploader un fichier CSV", "Utiliser les données du notebook (merged_resampled_1hz_no_t0.csv)"])
# 
#     df = None
#     if data_source == "Uploader un fichier CSV":
#         uploaded_file = st.file_uploader("Choisir un fichier CSV", type=["csv"])
#         if uploaded_file:
#             df = pd.read_csv(uploaded_file)
#             st.write("Fichier CSV chargé avec succès.")
#     else:
#         notebook_data_path = os.path.join(DRIVE_DATA_PATH, NOTEBOOK_DATA_FILE)
#         if os.path.exists(notebook_data_path):
#             df = pd.read_csv(notebook_data_path)
#             st.write("Données du notebook chargées avec succès.")
#         else:
#             st.error(f"Fichier {NOTEBOOK_DATA_FILE} non trouvé dans {DRIVE_DATA_PATH}.")
#             return
# 
#     if df is not None:
#         # Validate required columns
#         required_columns = ['cycle_id', 'Temps (s)'] + SENSOR_COLUMNS
#         missing_columns = [col for col in required_columns if col not in df.columns]
#         if missing_columns:
#             st.error(f"Colonnes manquantes dans les données : {', '.join(missing_columns)}")
#             return
# 
#         # Display data summary
#         st.subheader("Aperçu des données")
#         st.dataframe(df.head())
# 
#         # Correlation heatmap
#         st.subheader("Matrice de corrélation des capteurs")
#         fig = plot_correlation_heatmap(df, SENSOR_COLUMNS)
#         st.pyplot(fig)
# 
#         # Time series plot
#         st.subheader("Séries temporelles par cycle")
#         cycle_ids = df['cycle_id'].unique()
#         selected_cycle = st.selectbox("Choisir un cycle", cycle_ids)
#         fig = plot_time_series(df, SENSOR_COLUMNS, cycle_id=selected_cycle)
#         st.pyplot(fig)
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/pages/3_prediction.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# from utils import load_models, create_sequences, save_predictions, generate_pdf_report, SENSOR_COLUMNS, FEATURE_COLS, TARGET_COL, DRIVE_HISTORY_PATH, DRIVE_DATA_PATH, NOTEBOOK_DATA_FILE, plot_predictions
# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# import os
# 
# def main():
#     st.title("Prédiction")
#     st.header("Prédire SE avec vos données ou les données du notebook")
# 
#     data_source = st.radio("Source des données", ["Uploader un fichier CSV", "Utiliser les données du notebook (merged_resampled_1hz_no_t0.csv)"])
# 
#     df = None
#     if data_source == "Uploader un fichier CSV":
#         uploaded_file = st.file_uploader("Choisir un fichier CSV", type=["csv"])
#         if uploaded_file:
#             df = pd.read_csv(uploaded_file)
#     else:
#         notebook_data_path = os.path.join(DRIVE_DATA_PATH, NOTEBOOK_DATA_FILE)
#         if os.path.exists(notebook_data_path):
#             df = pd.read_csv(notebook_data_path)
#             st.write("Données du notebook chargées avec succès.")
#         else:
#             st.error("Fichier merged_resampled_1hz_no_t0.csv non trouvé sur Google Drive.")
#             return
# 
#     if df is not None:
#         if not all(col in df.columns for col in ['cycle_id', 'Temps (s)'] + SENSOR_COLUMNS):
#             st.error("Le fichier CSV doit contenir toutes les colonnes nécessaires.")
#             return
# 
#         models, scalers = load_models()  # Unpack the tuple
#         # Filter models to include only time-series models
#         time_series_models = {k: v for k, v in models.items() if k in ['lstm', 'gru', 'cnn-lstm']}
# 
#         if not time_series_models:
#             st.warning("Aucun modèle de série temporelle pré-entraîné trouvé. Réentraînez les modèles d'abord.")
#             return
# 
#         model_name = st.selectbox("Choisir un modèle", list(time_series_models.keys()))
#         seq_length = 10
# 
#         X, y, scaler = create_sequences(df, seq_length, TARGET_COL, FEATURE_COLS)
#         if X.shape[0] == 0:
#             st.error("Aucune séquence valide générée. Vérifiez vos données.")
#             return
# 
#         model = time_series_models[model_name]
#         try:
#             y_pred = model.predict(X, verbose=0)
#             y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
#             y_true = scaler.inverse_transform(y.reshape(-1, 1)).flatten()
# 
#             metrics = {
#                 'MSE': mean_squared_error(y_true, y_pred),
#                 'MAE': mean_absolute_error(y_true, y_pred),
#                 'R²': r2_score(y_true, y_pred)
#             }
# 
#             st.subheader("Résultats")
#             st.write(f"Métriques : MSE={metrics['MSE']:.4f}, MAE={metrics['MAE']:.4f}, R²={metrics['R²']:.4f}")
#             st.dataframe(pd.DataFrame({'Vrai SE': y_true, 'Prédit SE': y_pred}))
# 
#             # Visualisation
#             st.subheader("Visualisation des prédictions")
#             fig = plot_predictions(y_true, y_pred, title=f"Prédictions {model_name.upper()} vs Vraies Valeurs")
#             st.pyplot(fig)
# 
#             filename = f"prediction_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv"
#             save_predictions(y_pred, filename)
# 
#             st.download_button(
#                 label="Télécharger les prédictions (CSV)",
#                 data=pd.DataFrame({'Predicted_SE': y_pred}).to_csv(index=False).encode('utf-8'),
#                 file_name=filename,
#                 mime="text/csv"
#             )
# 
#             pdf_filename = filename.replace('.csv', '.pdf')
#             generate_pdf_report(y_pred, metrics, pdf_filename)
#             pdf_path = os.path.join(DRIVE_HISTORY_PATH, pdf_filename)
#             if os.path.exists(pdf_path):
#                 with open(pdf_path, 'rb') as f:
#                     st.download_button(
#                         label="Télécharger le rapport (PDF)",
#                         data=f,
#                         file_name=pdf_filename,
#                         mime="application/pdf"
#                     )
#         except Exception as e:
#             st.error(f"Erreur lors de la prédiction avec {model_name}: {str(e)}")
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/pages/4_retrain.py
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# from utils import build_lstm_model, build_gru_model, build_cnn_lstm_model, create_sequences, DRIVE_MODEL_PATH, SENSOR_COLUMNS, FEATURE_COLS, TARGET_COL
# import os
# 
# def main():
#     st.title("Réentraînement")
#     st.header("Uploader vos données pour réentraîner les modèles")
# 
#     uploaded_file = st.file_uploader("Choisir un fichier CSV", type=["csv"])
#     if uploaded_file:
#         df = pd.read_csv(uploaded_file)
#         if not all(col in df.columns for col in ['cycle_id', 'Temps (s)'] + SENSOR_COLUMNS):
#             st.error("Le fichier CSV doit contenir toutes les colonnes nécessaires.")
#             return
# 
#         seq_length = 10
#         X, y, scaler = create_sequences(df, seq_length, TARGET_COL, FEATURE_COLS)
#         train_size = int(0.8 * len(X))
#         X_train, X_test = X[:train_size], X[train_size:]
#         y_train, y_test = y[:train_size], y[train_size:]
# 
#         input_shape = (seq_length, len(FEATURE_COLS))
#         models = {
#             'lstm': build_lstm_model(input_shape),
#             'gru': build_gru_model(input_shape),
#             'cnn_lstm': build_cnn_lstm_model(input_shape)
#         }
# 
#         for name, model in models.items():
#             st.write(f"Entraînement du modèle {name.upper()}...")
#             model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=0)
#             model.save(os.path.join(DRIVE_MODEL_PATH, f'direct_{name}_se_pred_v1.keras'))
#             st.success(f"Modèle {name.upper()} entraîné et sauvegardé sur Google Drive.")
# 
#         st.success("Tous les modèles ont été réentraînés avec succès.")
# 
# if __name__ == "__main__":
#     main()
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/pages/5_history.py
# import streamlit as st
# import pandas as pd
# from utils import DRIVE_HISTORY_PATH
# import os
# 
# def main():
#     st.title("Historique")
#     st.header("Consulter les prédictions passées")
# 
#     files = [f for f in os.listdir(DRIVE_HISTORY_PATH) if f.endswith('.csv')]
#     if not files:
#         st.warning("Aucun fichier de prédiction trouvé.")
#         return
# 
#     selected_file = st.selectbox("Choisir un fichier", files)
#     df = pd.read_csv(os.path.join(DRIVE_HISTORY_PATH, selected_file))
#     st.dataframe(df)
# 
#     pdf_file = selected_file.replace('.csv', '.pdf')
#     pdf_path = os.path.join(DRIVE_HISTORY_PATH, pdf_file)
#     if os.path.exists(pdf_path):
#         with open(pdf_path, 'rb') as f:
#             st.download_button(
#                 label="Télécharger le rapport PDF",
#                 data=f,
#                 file_name=pdf_file,
#                 mime="application/pdf"
#             )
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/pages/6_compare.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# from utils import load_models, create_sequences, plot_comparison, plot_model_predictions, SENSOR_COLUMNS, FEATURE_COLS, TARGET_COL, DRIVE_DATA_PATH, NOTEBOOK_DATA_FILE
# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# import os
# 
# def main():
#     st.title("Comparateur")
#     st.header("Comparer les performances des modèles")
# 
#     data_source = st.radio("Source des données", ["Uploader un fichier CSV", "Utiliser les données du notebook (merged_resampled_1hz_no_t0.csv)"])
# 
#     df = None
#     if data_source == "Uploader un fichier CSV":
#         uploaded_file = st.file_uploader("Choisir un fichier CSV pour évaluation", type=["csv"])
#         if uploaded_file:
#             df = pd.read_csv(uploaded_file)
#     else:
#         notebook_data_path = os.path.join(DRIVE_DATA_PATH, NOTEBOOK_DATA_FILE)
#         if os.path.exists(notebook_data_path):
#             df = pd.read_csv(notebook_data_path)
#             st.write("Données du notebook chargées avec succès.")
#         else:
#             st.error("Fichier merged_resampled_1hz_no_t0.csv non trouvé sur Google Drive.")
#             return
# 
#     if df is not None:
#         if not all(col in df.columns for col in ['cycle_id', 'Temps (s)'] + SENSOR_COLUMNS):
#             st.error("Le fichier CSV doit contenir toutes les colonnes nécessaires.")
#             return
# 
#         seq_length = 10
#         X, y, scaler = create_sequences(df, seq_length, TARGET_COL, FEATURE_COLS)
#         models, scalers = load_models()  # Unpack the tuple
#         # Filter to time-series models
#         time_series_models = {k: v for k, v in models.items() if k in ['lstm', 'gru', 'cnn-lstm']}
# 
#         if not time_series_models:
#             st.warning("Aucun modèle de série temporelle pré-entraîné trouvé.")
#             return
# 
#         metrics_dict = {}
#         predictions_dict = {}
#         for name, model in time_series_models.items():
#             try:
#                 y_pred = model.predict(X, verbose=0)
#                 y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
#                 y_true = scaler.inverse_transform(y.reshape(-1, 1)).flatten()
#                 metrics_dict[name] = {
#                     'MSE': mean_squared_error(y_true, y_pred),
#                     'MAE': mean_absolute_error(y_true, y_pred),
#                     'R²': r2_score(y_true, y_pred)
#                 }
#                 predictions_dict[name] = y_pred
#             except Exception as e:
#                 st.error(f"Erreur lors de la prédiction avec {name}: {str(e)}")
#                 continue
# 
#         if metrics_dict:
#             st.subheader("Métriques")
#             st.dataframe(pd.DataFrame(metrics_dict).T)
# 
#             st.subheader("Visualisation des métriques")
#             fig = plot_comparison(metrics_dict)
#             st.pyplot(fig)
# 
#             st.subheader("Comparaison des prédictions")
#             fig = plot_model_predictions(y_true, predictions_dict)
#             st.pyplot(fig)
#         else:
#             st.error("Aucune prédiction valide n'a été générée.")
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/pages/0_tableau_de_bord.py
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# import plotly.graph_objects as go
# import plotly.express as px
# from utils import load_models, SENSOR_COLUMNS, DRIVE_DATA_PATH, NOTEBOOK_DATA_FILE
# import os
# 
# def main():
#     st.title("Tableau de Bord")
#     st.write("Vue d'ensemble de l'efficacité (SE) et des données des capteurs.")
# 
#     # Charger les données
#     data_path = os.path.join(DRIVE_DATA_PATH, NOTEBOOK_DATA_FILE)
#     if not os.path.exists(data_path):
#         st.error(f"Fichier {NOTEBOOK_DATA_FILE} non trouvé sur Google Drive.")
#         return
#     data = pd.read_csv(data_path)
# 
#     # Charger les modèles
#     models, scalers = load_models()
#     if not models or not scalers:
#         st.warning("Aucun modèle ou scaler chargé.")
#         return
# 
#     # Préparer les données pour la prédiction
#     sensor_columns = ['PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'CE']
#     if not all(col in data.columns for col in sensor_columns + ['SE']):
#         st.error("Les données doivent contenir toutes les colonnes nécessaires.")
#         return
# 
#     X = data[sensor_columns].values
#     scaler_X_se = scalers.get('scaler_X_se')
#     scaler_y_se = scalers.get('scaler_y_se')
#     if not scaler_X_se or not scaler_y_se:
#         st.error("Scalers pour SE non chargés.")
#         return
# 
#     X_scaled = scaler_X_se.transform(X)
#     X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))
# 
#     # Prédictions
#     se_pred_lstm = None
#     se_pred_gru = None
#     if 'lstm_se' in models:
#         se_scaled = models['lstm_se'].predict(X_reshaped, verbose=0)
#         se_pred_lstm = np.clip(scaler_y_se.inverse_transform(se_scaled).flatten(), 0, 100)
#     if 'gru_se' in models:
#         se_scaled = models['gru_se'].predict(X_reshaped, verbose=0)
#         se_pred_gru = np.clip(scaler_y_se.inverse_transform(se_scaled).flatten(), 0, 100)
# 
#     # Plot SE
#     fig = go.Figure()
#     fig.add_trace(go.Scatter(x=data.index, y=data['SE'], mode='lines', name='Efficacité Réelle (SE)'))
#     if se_pred_lstm is not None:
#         fig.add_trace(go.Scatter(x=data.index, y=se_pred_lstm, mode='lines', name='Efficacité Prédite (LSTM)', line=dict(dash='dash')))
#     if se_pred_gru is not None:
#         fig.add_trace(go.Scatter(x=data.index, y=se_pred_gru, mode='lines', name='Efficacité Prédite (GRU)', line=dict(dash='dot')))
#     fig.update_layout(title='Efficacité Réelle vs Prédite par Cycle', xaxis_title='Cycle', yaxis_title='SE (%)')
#     st.plotly_chart(fig)
# 
#     # Heatmap de corrélation
#     st.subheader("Corrélations des Capteurs")
#     corr_cols = ['SE', 'PS1', 'FS1', 'TS1', 'CE']
#     corr = data[corr_cols].corr()
#     fig = go.Figure(data=go.Heatmap(z=corr.values, x=corr.columns, y=corr.columns, colorscale='Viridis'))
#     st.plotly_chart(fig)
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/pages/1_chatbot.py
# import streamlit as st
# from utils import load_models, HydraulicNLPAssistant
# 
# def main():
#     st.title("Chatbot")
#     st.write("Interagissez avec l'assistant pour optimiser l'efficacité ou prédire SE.")
# 
#     # Charger les modèles
#     models, scalers = load_models()
#     assistant = HydraulicNLPAssistant(models, scalers)
# 
#     user_input = st.text_input("Entrez votre message:", "")
#     if st.button("Envoyer"):
#         if user_input:
#             response = assistant.process_message(user_input)
#             if response['type'] == 'prediction':
#                 st.success(response['message'])
#                 if response.get('warnings'):
#                     st.warning("\n".join(response['warnings']))
#             elif response['type'] == 'optimization':
#                 st.success(response['message'])
#                 if response.get('sensor_values'):
#                     st.write("Valeurs des capteurs:", response['sensor_values'])
#                 if response.get('warnings'):
#                     st.warning("\n".join(response['warnings']))
#             elif response['type'] == 'error':
#                 st.error(response['message'])
#             else:
#                 st.info(response['message'])
#         else:
#             st.warning("Veuillez entrer un message.")
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/pages/2_predict_se.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import plotly.graph_objects as go
# from utils import load_models
# 
# def main():
#     st.title("Prédiction de l'Efficacité Système (SE)")
#     st.write("Entrez les valeurs des capteurs pour prédire SE avec LSTM et GRU.")
# 
#     # Charger les modèles
#     models, scalers = load_models()
#     sensor_columns = ['PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'CE']
# 
#     sensor_values = []
#     for sensor in sensor_columns:
#         value = st.number_input(f"Valeur pour {sensor}", value=0.0, step=0.1)
#         sensor_values.append(value)
# 
#     if st.button("Prédire SE"):
#         if len(sensor_values) == len(sensor_columns):
#             scaler_X_se = scalers.get('scaler_X_se')
#             scaler_y_se = scalers.get('scaler_y_se')
#             if not scaler_X_se or not scaler_y_se:
#                 st.error("Scalers pour SE non chargés.")
#                 return
# 
#             input_df = pd.DataFrame([sensor_values], columns=sensor_columns)
#             input_scaled = scaler_X_se.transform(input_df)
#             input_reshaped = input_scaled.reshape((1, 1, input_scaled.shape[1]))
# 
#             se_lstm = None
#             se_gru = None
#             if 'lstm_se' in models:
#                 se_lstm_scaled = models['lstm_se'].predict(input_reshaped, verbose=0)
#                 se_lstm = scaler_y_se.inverse_transform(se_lstm_scaled)[0][0]
#             if 'gru_se' in models:
#                 se_gru_scaled = models['gru_se'].predict(input_reshaped, verbose=0)
#                 se_gru = scaler_y_se.inverse_transform(se_gru_scaled)[0][0]
# 
#             if se_lstm is not None or se_gru is not None:
#                 if se_lstm is not None:
#                     st.success(f"Prédiction SE (LSTM): {se_lstm:.2f}%")
#                 if se_gru is not None:
#                     st.success(f"Prédiction SE (GRU): {se_gru:.2f}%")
# 
#                 fig = go.Figure()
#                 if se_lstm is not None:
#                     fig.add_trace(go.Bar(x=['LSTM'], y=[se_lstm], name='LSTM'))
#                 if se_gru is not None:
#                     fig.add_trace(go.Bar(x=['GRU'], y=[se_gru], name='GRU'))
#                 fig.update_layout(title='Comparaison des Prédictions SE', yaxis_title='SE (%)', yaxis_range=[0, 100])
#                 st.plotly_chart(fig)
#             else:
#                 st.error("Aucun modèle LSTM ou GRU chargé.")
#         else:
#             st.error(f"Veuillez fournir {len(sensor_columns)} valeurs de capteurs.")
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_app/pages/3_predict_inverse.py
# import streamlit as st
# import numpy as np
# from utils import load_models
# 
# def main():
#     st.title("Prédiction Inverse (SE vers Capteurs)")
#     st.write("Entrez une valeur cible pour SE pour prédire les valeurs des capteurs.")
# 
#     # Charger les modèles
#     models, scalers = load_models()
#     sensor_columns = ['PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'CE']
# 
#     target_se = st.number_input("Cible SE (%)", min_value=0.0, max_value=100.0, value=50.0)
#     if st.button("Prédire Capteurs"):
#         if 'mlp_inverse' not in models:
#             st.error("Modèle MLP inverse non chargé.")
#             return
#         scaler_X_mlp = scalers.get('scaler_X_inverse')
#         scaler_y_mlp = scalers.get('scaler_y_inverse')
#         if not scaler_X_mlp or not scaler_y_mlp:
#             st.error("Scalers pour prédiction inverse non chargés.")
#             return
# 
#         se_scaled = scaler_X_mlp.transform(np.array([[target_se]]))
#         sensors_scaled = models['mlp_inverse'].predict(se_scaled)
#         sensors_original = scaler_y_mlp.inverse_transform(sensors_scaled)[0]
#         sensor_dict = {col: round(val, 2) for col, val in zip(sensor_columns, sensors_original)}
# 
#         warnings = []
#         for col, val in sensor_dict.items():
#             if col.startswith('PS') and val < 0:
#                 warnings.append(f"Avertissement: {col} pression ({val:.2f}) est négative.")
#             elif col.startswith('TS') and (val < 0 or val > 100):
#                 warnings.append(f"Avertissement: {col} température ({val:.2f}) hors plage 0-100°C.")
#             elif col == 'CE' and (val < 0 or val > 100):
#                 warnings.append(f"Avertissement: {col} efficacité ({val:.2f}) hors plage 0-100%.")
# 
#         st.success(f"Prédiction des valeurs des capteurs pour SE = {target_se}%:")
#         st.write(sensor_dict)
#         if warnings:
#             st.warning("\n".join(warnings))
# 
# if __name__ == "__main__":
#     main()

# Obtenir l'adresse IP publique
!wget -q -O - ipv4.icanhazip.com

# Installer localtunnel
!npm install localtunnel

# Lancer Streamlit en arrière-plan
!streamlit run /content/streamlit_app/app.py &>/content/streamlit_app/streamlit.log &


!npx localtunnel --port 8501

