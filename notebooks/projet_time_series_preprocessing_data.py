# -*- coding: utf-8 -*-
"""projet_time_series_preprocessing_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QIkCkvf14PerGLqx0wJWca65qPhNy0aI

# **Projet de Séries Temporelles : Prédiction de l'Efficacité et Identification des Paramètres Optimaux d'un Système Hydraulique**

## **1. Introduction**
Ce projet de **maintenance prédictive** utilise l'analyse des séries temporelles collectées par des capteurs afin de prédire l'efficacité et identifier les paramètres optimaux d'un **système hydraulique**. L'objectif est de maximiser la performance du système tout en minimisant les coûts d'exploitation et de maintenance.

L'**analyse des séries temporelles** nous permettra de prédire la performance du système, d'identifier les anomalies et les dégradations potentielles des composants, ainsi que d'optimiser les réglages opérationnels pour garantir une efficacité optimale.

---

## **Qu'est-ce qu'un Système Hydraulique ?**
Un **système hydraulique** est un ensemble d'éléments interconnectés qui utilisent un fluide sous pression (généralement de l'huile) pour transmettre de l'énergie et accomplir des travaux mécaniques. Il est largement utilisé dans diverses applications industrielles telles que les machines de construction, les presses hydrauliques, et les véhicules lourds.

### **Composants Principaux** :
1. **Pompe Hydraulique** : Fournit un débit constant de fluide sous pression.
2. **Cylindre ou Moteur Hydraulique** : Convertit l'énergie hydraulique en mouvement mécanique.
3. **Vannes** : Permettent de contrôler la direction et la pression du fluide dans le système.
4. **Réservoir** : Stocke et restitue le fluide utilisé dans le système.
5. **Filtre** : Maintient la propreté du fluide en éliminant les impuretés.

Les composants critiques, tels que les pompes et les vannes, sont particulièrement sensibles aux dégradations, ce qui peut affecter la performance globale du système.

---

## **Pourquoi l'Efficacité est-elle Cruciale ?**
L'efficacité d'un système hydraulique est essentielle pour garantir :
- **Réduction de la consommation énergétique** et des coûts opérationnels.
- **Prolongation de la durée de vie des équipements** en minimisant l'usure prématurée des composants.
- **Minimisation des déchets et des émissions**, contribuant ainsi à un fonctionnement durable.

Des problèmes tels que les **fuites**, l'**usure des pompes**, et les **blocages de vannes** peuvent réduire l'efficacité du système. C'est pourquoi la surveillance en temps réel et la maintenance préventive sont cruciales pour maintenir des performances optimales.

---

## **2. Aperçu du Jeu de Données**
Les données utilisées dans ce projet proviennent de plusieurs capteurs installés sur le système hydraulique. Ces capteurs mesurent des paramètres clés tels que la **pression**, le **débit**, la **température**, et les **vibrations**. Ces données nous permettent d'analyser l'efficacité du système et d'identifier les paramètres optimaux pour améliorer sa performance.

### **Variables Cibles** :
- **Efficacité (SE)** : Efficacité globale du système hydraulique, mesurée en pourcentage.
- **Efficacité du Refroidissement (CE)** : Efficacité du système de refroidissement, un facteur clé pour éviter la surchauffe des composants.
- **État des Composants** : Condition de santé des composants critiques (pompe, vannes, etc.), indiquant si une maintenance préventive est nécessaire.

### **Aperçu des Données des Capteurs** :

| **Capteur**  | **Description** | **Fréquence** | **Points/Cycle** | **Fichier de Données** |
|--------------|-----------------|---------------|------------------|------------------------|
| **PS1-PS6**  | Pression dans différentes sections du circuit | 100 Hz | 6000 | `PS1.txt` à `PS6.txt` |
| **EPS1**     | Puissance générée par la pompe (kW) | 100 Hz | 6000 | `EPS1.txt` |
| **FS1-FS2**  | Débit du fluide dans le système (L/s) | 10 Hz | 600 | `FS1.txt`, `FS2.txt` |
| **TS1-TS4**  | Température du fluide (°C) à différents points | 1 Hz | 60 | `TS1.txt` à `TS4.txt` |
| **VS1**      | Vibrations du moteur/pompe (mm/s) | 1 Hz | 60 | `VS1.txt` |
| **CE**       | Efficacité thermique du système hydraulique (%) | 1 Hz | 60 | `CE.txt` |
| **SE**       | Rendement global du système hydraulique (%) | 1 Hz | 60 | `SE.txt` |
*Note : PS4 exclu - données non valides*


Ces données de capteurs nous permettent de modéliser l'efficacité du système et d'identifier les réglages optimaux pour une performance maximale.

### **Profil d'État des Composants**

Le fichier `profile.txt` contient des informations cruciales sur l'état des composants hydrauliques pour chaque cycle. Il contient les colonnes suivantes :

- **Cooler_condition** : État du système de refroidissement.
- **Valve_condition** : État des vannes.
- **Internal_pump_leakage** : Niveau de fuite interne de la pompe.
- **Hydraulic_accumulator** : État de l'accumulateur hydraulique.
- **Stable_flag** : Indicateur de stabilité du cycle.

Ces variables seront utilisées comme labels pour des tâches de classification ou comme indicateurs pour la détection de défaillance.
 En analysant les séries temporelles de ces capteurs, nous pourrons prédire l'efficacité future, détecter les anomalies et ajuster les paramètres du système pour éviter les pannes et optimiser la consommation énergétique.

---

## **3. Méthodologie d'Analyse**
### **Préparation des Données** :
- Nettoyage des données (gestion des valeurs manquantes, élimination des anomalies, etc.).
- Transformation des séries temporelles pour extraire des caractéristiques (moyenne, écart type, etc.).
  
### **Modélisation Prédictive** :
- **Modèles de Séries Temporelles** : Nous utiliserons des modèles comme **ARIMA**, **SARIMAX** et **LSTM** pour analyser les séries temporelles et prédire les comportements futurs du système hydraulique.
- **Optimisation des Paramètres** : L'objectif est de trouver les paramètres optimaux pour maintenir l'efficacité et prolonger la durée de vie du système.

Les prédictions générées par ces modèles nous permettront d'agir proactivement pour éviter des défaillances coûteuses et garantir des performances maximales.

---

### **Conclusion** :
Ce projet vise à appliquer des techniques avancées d'analyse de séries temporelles pour améliorer la performance des systèmes hydrauliques. Grâce à l'utilisation de modèles prédictifs et à l'identification des paramètres clés, nous pourrons non seulement prédire l'efficacité mais aussi optimiser les paramètres du système en temps réel pour garantir une meilleure performance et une maintenance proactive.

# Chargement et prétraitement des données
Chargez différents fichiers de données de capteurs (PS1-6, EPS1, FS1-2, TS1-4, VS1, CE, CP, SE) et profile.txt. Gérez différents taux d'échantillonnage et alignez les données de séries chronologiques.
"""

from google.colab import drive
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

# Monter le drive
drive.mount('/content/drive')

# chemin des données
data_path = '/content/drive/MyDrive/Projet_time_series/Data_txt/data_text'

# Verifications des fichiers
file_list = os.listdir(data_path)
print("Les fichiers qui se trouvent sont :")
for i, file in enumerate(file_list, 1):
    print(f"{i}. {file}")

input_path = '/content/drive/MyDrive/Projet_time_series/Data_txt/data_text'
output_path = '/content/drive/MyDrive/Projet_time_series/Data_csv'

# Créer le dossier de sortie
os.makedirs(output_path, exist_ok=True)

sensors = {
    'PS1': 6000, 'PS2': 6000, 'PS3': 6000, 'PS5': 6000, 'PS6': 6000,
    'EPS1': 6000,
    'FS1': 600, 'FS2': 600,
    'TS1': 60, 'TS2': 60, 'TS3': 60, 'TS4': 60,
    'VS1': 60,
    'CE': 60, 'CP': 60, 'SE': 60
}

# Conversion pour chaque capteur
for sensor, n_points in sensors.items():
    # Chargement du fichier texte
    df = pd.read_csv(f'{input_path}/{sensor}.txt', sep='\t', header=None, engine='python')

    # Nommage des colonnes : "Cycle_X" pour Xème point du cycle
    df.columns = [f'{sensor}_Point_{i+1}' for i in range(n_points)]

    # Sauvegarde en CSV
    df.to_csv(f'{output_path}/{sensor}.csv', index=False)
    print(f'{sensor}.csv converti ({df.shape[0]} cycles, {df.shape[1]} points/cycle)')

# Conversion du profil
profile = pd.read_csv(f'{input_path}/profile.txt', sep='\t', header=None)
profile.columns = ['Cooler_condition', 'Valve_condition', 'Internal_pump_leakage',
                   'Hydraulic_accumulator', 'Stable_flag']
profile.to_csv(f'{output_path}/profile.csv', index=False)
print("profile.csv converti")

# Vérification  du contenu des fichiers convertis et s'assurer de l'inexistance de valeur NaN
for sensor in sensors.keys():
    df = pd.read_csv(f'{output_path}/{sensor}.csv')
    print(f'{sensor}.csv -> Shape: {df.shape}, NaN values: {df.isna().sum().sum()}')

# Vérification du profil
profile = pd.read_csv(f'{output_path}/profile.csv')
print(f'profile.csv -> Shape: {profile.shape}, NaN values: {profile.isna().sum().sum()}')

# Chemin des fichiers convertis
output_path = '/content/drive/MyDrive/Projet_time_series/Data_csv'
long_format_path = '/content/drive/MyDrive/Projet_time_series/Data_long'

# Création du dossier de sortie
os.makedirs(long_format_path, exist_ok=True)

# Fréquences des capteurs (en Hz)
sampling_rates = {
    'PS1': 100, 'PS2': 100, 'PS3': 100, 'PS5': 100, 'PS6': 100,
    'EPS1': 100,
    'FS1': 10, 'FS2': 10,
    'TS1': 1, 'TS2': 1, 'TS3': 1, 'TS4': 1,
    'VS1': 1,
    'CE': 1, 'CP': 1, 'SE': 1
}

# Conversion en format long
for sensor, freq in sampling_rates.items():
    file_path = f'{output_path}/{sensor}.csv'

    # Charger les données
    df = pd.read_csv(file_path)

    # Ajout d'un ID de cycle
    df['cycle_id'] = df.index

    # Transformation en format long
    columns = [col for col in df.columns if col != 'cycle_id']
    df_long = pd.melt(df, id_vars=['cycle_id'], value_vars=columns, var_name='Point', value_name='Valeur')

    # Extraire le numéro du point à partir du nom de colonne
    df_long['point_num'] = df_long['Point'].str.extract(r'Point_(\d+)').astype(int)

    # Calculer le temps correct en fonction du numéro du point et de la fréquence
    df_long['Temps (s)'] = (df_long['point_num'] - 1) / freq + df_long.groupby('cycle_id')['point_num'].transform('min') * (1 / freq)

    # Ajouter le nom du capteur
    df_long['Capteur'] = sensor

    # Réorganiser les colonnes
    df_long = df_long[['cycle_id', 'Temps (s)', 'Capteur', 'Point', 'Valeur']]

    # Sauvegarde
    df_long.to_csv(f'{long_format_path}/{sensor}_long.csv', index=False)
    print(f'{sensor}_long.csv ')

print("Conversion des fichiers en format long ")

"""Verifiant le bon transformation"""

# Vérification des premières lignes du fichier transformé en format long
for sensor in sensors.keys():
    df_long = pd.read_csv(f'{long_format_path}/{sensor}_long.csv')
    print(f"\n{sensor}_long.csv - Premiere ligne :")
    print(df_long.head())

"""## **EDA (Analyse exploratoire des données):**
L'EDA te permettra de mieux comprendre la structure et les tendances de tes séries temporelles avant d'entamer la modélisation.<br>
###**Statistiques de base**
* Moyenne, médiane, écart-type, min, max pour chaque capteur.
* Distribution des valeurs avec des histogrammes.
### **Visualisation des séries temporelles**
Tracer les courbes des capteurs en fonction du temps.

Comparer les capteurs entre eux pour détecter d’éventuelles corrélations.

### **Détection des valeurs aberrantes**
* Boîtes à moustaches (boxplots) pour repérer des outliers.
* Détection par z-score ou IQR (Interquartile Range).

### **Analyse de la tendance et de la saisonnalité**
* Décomposition des séries temporelles (tendance, saisonnalité, bruit).
* Calcul de l'auto-corrélation (ACF/PACF) pour voir les relations temporelles.

### **Analyse des lacunes dans les données**
* Vérifier s'il y a des valeurs manquantes (déja fait).
"""

capteurs_a_analyser = ['PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'CE', 'SE']

#Statistiques de base pour chaque capteur
def analyser_statistiques_capteur(sensor_name):
    fichier = f'{long_format_path}/{sensor_name}_long.csv'
    df = pd.read_csv(fichier)

    # Statistiques de base
    stats = df.groupby('cycle_id')['Valeur'].agg(['mean', 'median', 'std', 'min', 'max'])
    print(f"\nStatistiques pour {sensor_name}:")
    print(stats.describe())

    return stats

for sensor in capteurs_a_analyser:
    stats = analyser_statistiques_capteur(sensor)

# Visualisation des séries temporelles pour un cycle spécifique
def visualiser_serie_temporelle(sensor_name, cycle_id=0):
    fichier = f'{long_format_path}/{sensor_name}_long.csv'
    df = pd.read_csv(fichier)

    # Filtrer pour le cycle spécifique
    cycle_data = df[df['cycle_id'] == cycle_id]

    plt.figure(figsize=(12, 6))
    plt.plot(cycle_data['Temps (s)'], cycle_data['Valeur'])
    plt.title(f'Série temporelle du capteur {sensor_name} - Cycle {cycle_id}')
    plt.xlabel('Temps (s)')
    plt.ylabel('Valeur')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

for sensor in capteurs_a_analyser:
    visualiser_serie_temporelle(sensor, cycle_id=0)

def comparer_capteurs(sensor_list, cycle_id=0):
    plt.figure(figsize=(14, 8))

    for sensor in sensor_list:
        fichier = f'{long_format_path}/{sensor}_long.csv'
        df = pd.read_csv(fichier)

        # Filtrer pour le cycle spécifique
        cycle_data = df[df['cycle_id'] == cycle_id]

        # Valeurs brutes
        plt.plot(cycle_data['Temps (s)'], cycle_data['Valeur'], label=f'{sensor} (brut)')

        # Normalisation des valeurs pour une meilleure comparaison
        min_val = cycle_data['Valeur'].min()
        max_val = cycle_data['Valeur'].max()
        if max_val > min_val:  # Éviter division par zéro
            normalized_values = (cycle_data['Valeur'] - min_val) / (max_val - min_val)
            plt.plot(cycle_data['Temps (s)'], normalized_values, label=f'{sensor} (normalisé)')

    plt.title(f'Comparaison des capteurs - Cycle {cycle_id}')
    plt.xlabel('Temps (s)')
    plt.ylabel('Valeur')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
for sensor in capteurs_a_analyser:
    comparer_capteurs([sensor], cycle_id=0)

# Détection des valeurs aberrantes avec boxplot
def detecter_outliers(sensor_name):

    fichier = f'{long_format_path}/{sensor_name}_long.csv'
    df = pd.read_csv(fichier)

    # Calculer les statistiques par cycle
    stats_by_cycle = df.groupby('cycle_id')['Valeur'].agg(['mean', 'std', 'min', 'max'])

    plt.figure(figsize=(12, 6))
    sns.boxplot(data=stats_by_cycle)
    plt.title(f'Distribution des statistiques par cycle - {sensor_name}')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Détection par méthode IQR
    Q1 = stats_by_cycle['mean'].quantile(0.25)
    Q3 = stats_by_cycle['mean'].quantile(0.75)
    IQR = Q3 - Q1

    outliers = stats_by_cycle[(stats_by_cycle['mean'] < (Q1 - 1.5 * IQR)) |
                             (stats_by_cycle['mean'] > (Q3 + 1.5 * IQR))]

    if not outliers.empty:
        print(f"\nCycles potentiellement aberrants pour {sensor_name}:")
        print(outliers)
    else:
        print(f"\nAucun cycle aberrant détecté pour {sensor_name}")

    return outliers

for sensor in capteurs_a_analyser:
    detecter_outliers(sensor)

"""![image.png](data:image/png;base64,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)"""

# Analyse de corrélation entre capteurs
def analyser_correlations_capteurs(sensor_list):
    # Créer un DataFrame pour stocker les moyennes des capteurs par cycle
    means_df = pd.DataFrame(index=range(2205))

    for sensor in sensor_list:
        fichier = f'{long_format_path}/{sensor}_long.csv'
        df = pd.read_csv(fichier)

        # Calculer la moyenne par cycle
        cycle_means = df.groupby('cycle_id')['Valeur'].mean()
        means_df[sensor] = cycle_means.values

    # Calculer la matrice de corrélation
    corr_matrix = means_df.corr()

    plt.figure(figsize=(12, 10))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')
    plt.title('Matrice de corrélation entre capteurs')
    plt.tight_layout()
    plt.show()

    return corr_matrix

analyser_correlations_capteurs(capteurs_a_analyser)

#Analyse de la relation avec se
def analyser_relation_se(sensor_list):
    # Charger les données d'efficacite
    se_file = f'{long_format_path}/SE_long.csv'
    se_df = pd.read_csv(se_file)
    se_means = se_df.groupby('cycle_id')['Valeur'].mean()

    # Créer un DataFrame pour l'analyse
    analysis_df = pd.DataFrame({'SE': se_means.values})


    for sensor in sensor_list:
        if sensor != 'SE':
            fichier = f'{long_format_path}/{sensor}_long.csv'
            df = pd.read_csv(fichier)
            cycle_means = df.groupby('cycle_id')['Valeur'].mean()
            analysis_df[sensor] = cycle_means.values

    # Créer des visualisations
    plt.figure(figsize=(18, 12))
    for i, sensor in enumerate(sensor_list):
        if sensor != 'SE':
            plt.subplot(3, 4, i+1)
            plt.scatter(analysis_df[sensor], analysis_df['SE'], alpha=0.5)
            plt.title(f'{sensor} vs SE')
            plt.xlabel(sensor)
            plt.ylabel('System Efficiency (SE)')
            plt.grid(True)

    plt.tight_layout()
    plt.show()

    # Calculer les corrélations avec SE
    correlations_with_se = analysis_df.corr()['SE'].sort_values(ascending=False)
    print("\nCorrélations avec l'efficacité du système (SE):")
    print(correlations_with_se)

    return correlations_with_se

analyser_relation_se(capteurs_a_analyser)

# Relation entre l'état des composants et l'efficacité
def analyser_etat_composants():


    profile = pd.read_csv(f'{output_path}/profile.csv')


    se_file = f'{long_format_path}/SE_long.csv'
    se_df = pd.read_csv(se_file)
    se_means = se_df.groupby('cycle_id')['Valeur'].mean().reset_index()
    se_means.columns = ['cycle_id', 'SE_mean']

    # Fusionner
    combined = pd.merge(profile, se_means, left_index=True, right_on='cycle_id')

    # Visualiser l'efficacité par état des composants
    plt.figure(figsize=(16, 10))

    components = ['Cooler_condition', 'Valve_condition', 'Internal_pump_leakage', 'Hydraulic_accumulator']
    for i, component in enumerate(components):
        plt.subplot(2, 2, i+1)
        sns.boxplot(x=component, y='SE_mean', data=combined)
        plt.title(f'Efficacité par état de {component}')
        plt.xlabel(component)
        plt.ylabel('Efficacité moyenne (SE)')
        plt.grid(True)

    plt.tight_layout()
    plt.show()

    # Analyse statistique
    for component in components:
        print(f"\nAnalyse pour {component}:")
        print(combined.groupby(component)['SE_mean'].agg(['count', 'mean', 'std', 'min', 'max']))

analyser_etat_composants()

"""## Analyse de la Matrice de Corrélation

### Fortes corrélations positives

- **PS1 et PS2 (0.995)** : Ces capteurs de pression sont presque parfaitement corrélés.
- **PS5 et PS6 (0.99999)** : Corrélation quasi parfaite, indiquant une redondance .
- **FS1 et SE (0.995)** : Le débit FS1 est fortement lié à l'efficacité du système.

---

### Fortes corrélations négatives

- **PS1 et FS1 (-0.924)** : Relation inverse forte entre pression d’entrée et débit.
- **PS2 et SE (-0.958)** : Pression PS2 négativement corrélée à l’efficacité.
- **PS1 et SE (-0.944)** : Similaire à PS2.
- **TS1 et PS5/PS6 (≈ -0.993)** : La température est inversement liée à ces pressions.

---

##  Capteurs les plus influents sur l'efficacité (SE)

### Corrélations positives avec SE :

- **FS1 (0.995)** : Le débit est le facteur le plus positivement corrélé à l’efficacité.
- **PS3 (0.884)** : Forte corrélation positive.
- **FS2 (0.466)** : Corrélation modérée positive.

### Corrélations négatives avec SE :

- **PS2 (-0.958)** : Très forte corrélation négative.
- **PS1 (-0.944)** : Très forte corrélation négative.
- **EPS1 (-0.678)** : Corrélation négative significative.

---

##  Impact de l'état des composants sur l'efficacité

###  Refroidisseur (Cooler_condition) :
- L’état **dégradé (3)** montre une efficacité plus faible (49.83) avec forte variabilité (écart-type 13.88).
- Les états **20 et 100** maintiennent une efficacité élevée et stable (~58 avec écart-type ~1.5).

###  Valve (Valve_condition) :
- L’état **optimal (100)** montre la meilleure efficacité moyenne (57.43).
- Les états **73, 80 et 90** présentent des efficacités similaires mais inférieures (~53).

###  Fuite de pompe interne (Internal_pump_leakage) :
- L’absence de fuite (**0**) correspond à la meilleure efficacité (59.16).
- Les niveaux de fuite **1 et 2** réduisent significativement l’efficacité (51.47 et 49.50).

###  Accumulateur hydraulique (Hydraulic_accumulator) :
- La valeur **130** montre la meilleure efficacité (58.47) avec faible variabilité.
- Les valeurs **90, 100 et 115** montrent des efficacités plus faibles.

---

##  Conclusions

###  Indicateurs clés de performance :
- Le **débit FS1** est l’indicateur le plus fiable de l’efficacité du système.
- Les **pressions PS1 et PS2** sont d’excellents indicateurs inverses de l’efficacité.

###  Composants critiques :
- La **fuite de pompe interne** a un impact majeur sur l’efficacité.
- L’état du **refroidisseur** influence significativement la stabilité du système.





"""

from statsmodels.tsa.seasonal import seasonal_decompose
import statsmodels.api as sm

def decomposer_serie_temporelle(sensor_name, cycle_id=0, freq=None):

    # Charger les données
    fichier = f'{long_format_path}/{sensor_name}_long.csv'
    df = pd.read_csv(fichier)

    # Filtrer pour le cycle spécifique et trier par temps
    cycle_data = df[df['cycle_id'] == cycle_id].sort_values('Temps (s)')

    # Créer une série temporelle
    ts = pd.Series(cycle_data['Valeur'].values, index=pd.to_datetime(cycle_data['Temps (s)'], unit='s'))

    # Déterminer la fréquence si non spécifiée
    if freq is None:
        # Auto-détection basée sur l'analyse spectrale
        from scipy import signal
        f, Pxx = signal.periodogram(ts)

        # Exclure la fréquence zéro
        if len(f) > 1:
            peak_idx = np.argmax(Pxx[1:]) + 1
            freq = int(1/f[peak_idx])
            freq = max(2, min(len(ts)//3, freq))
        else:
            freq = len(ts) // 10

    # Décomposition
    try:
        decomposition = seasonal_decompose(ts, model='additive', period=freq)

        # Visualisation
        plt.figure(figsize=(14, 12))

        plt.subplot(411)
        plt.plot(ts.index, ts.values)
        plt.title(f'Série originale - {sensor_name}, Cycle {cycle_id}')
        plt.grid(True)

        plt.subplot(412)
        plt.plot(decomposition.trend)
        plt.title('Tendance')
        plt.grid(True)

        plt.subplot(413)
        plt.plot(decomposition.seasonal)
        plt.title(f'Saisonnalité (période={freq})')
        plt.grid(True)

        plt.subplot(414)
        plt.plot(decomposition.resid)
        plt.title('Résidus')
        plt.grid(True)

        plt.tight_layout()
        plt.show()

        return decomposition

    except Exception as e:
        print(f"Erreur lors de la décomposition: {e}")
        print(f"Essayez avec : {freq}")
        return None

def analyser_autocorrelation(sensor_name, cycle_id=0, lags=None):
    # Charger les données
    fichier = f'{long_format_path}/{sensor_name}_long.csv'
    df = pd.read_csv(fichier)

    # Filtrer pour le cycle spécifique
    cycle_data = df[df['cycle_id'] == cycle_id].sort_values('Temps (s)')

    # Déterminer le nombre de lags si non spécifié
    if lags is None:
        lags = min(40, len(cycle_data) // 2 - 1)  # Limite à 50% de  l'échantillon

    # Calculer l'ACF et PACF
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    # ACF
    sm.graphics.tsa.plot_acf(cycle_data['Valeur'].values, lags=lags, ax=ax1)
    ax1.set_title(f'Autocorrélation (ACF) - {sensor_name}, Cycle {cycle_id}')

    # PACF
    sm.graphics.tsa.plot_pacf(cycle_data['Valeur'].values, lags=lags, ax=ax2)
    ax2.set_title(f'Autocorrélation partielle (PACF) - {sensor_name}, Cycle {cycle_id}')

    plt.tight_layout()
    plt.show()

for sensor in capteurs_a_analyser:
    decomposer_serie_temporelle(sensor, cycle_id=0)
    analyser_autocorrelation(sensor, cycle_id=0)

def fusionner_donnees_capteurs(sensor_list):


    all_data = pd.DataFrame()

    for sensor in sensor_list:

        fichier = f'{long_format_path}/{sensor}_long.csv'
        df = pd.read_csv(fichier)


        df = df.rename(columns={'Valeur': sensor})

        # Si c'est le premier capteur, garder toutes les colonnes
        if all_data.empty:
            all_data = df
        else:
            # Sinon, ne garder que cycle_id, temps et la valeur
            all_data = pd.merge(
                all_data,
                df[['cycle_id', 'Temps (s)', sensor]],
                on=['cycle_id', 'Temps (s)'],
                how='outer'
            )

    # Trier les données
    all_data = all_data.sort_values(['cycle_id', 'Temps (s)'])

    # Sauvegarder le fichier fusionné
    output_file = f'{output_path}/all_sensors_data.csv'
    all_data.to_csv(output_file, index=False)
    print(f"Données fusionnées sauvegardées dans : {output_file}")

    return all_data


all_sensors_data = fusionner_donnees_capteurs(capteurs_a_analyser)

def analyser_tendances_long_terme(all_data, sensor_list):


    plt.figure(figsize=(15, 10))

    for sensor in sensor_list:
        # Calculer la moyenne mobile sur 100 cycles
        rolling_mean = all_data.groupby('cycle_id')[sensor].mean().rolling(window=100).mean()

        plt.plot(rolling_mean.index, rolling_mean.values, label=sensor)

    plt.title('Tendances à long terme des capteurs')
    plt.xlabel('Cycle')
    plt.ylabel('Valeur moyenne')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Analyser les tendances
analyser_tendances_long_terme(all_sensors_data, capteurs_a_analyser)

def detecter_anomalies(all_data, sensor_list, window=100, threshold=3):

    anomalies = {}

    for sensor in sensor_list:
        # Calculer la moyenne mobile et l'écart-type mobile
        rolling_mean = all_data.groupby('cycle_id')[sensor].mean().rolling(window=window).mean()
        rolling_std = all_data.groupby('cycle_id')[sensor].mean().rolling(window=window).std()

        # Identifier les anomalies
        sensor_data = all_data.groupby('cycle_id')[sensor].mean()
        z_scores = abs((sensor_data - rolling_mean) / rolling_std)
        anomalies[sensor] = z_scores[z_scores > threshold]

    # Visualiser les anomalies
    plt.figure(figsize=(15, 10))
    for sensor in sensor_list:
        plt.scatter(
            anomalies[sensor].index,
            all_data.groupby('cycle_id')[sensor].mean()[anomalies[sensor].index],
            label=f'{sensor} anomalies'
        )

    plt.title('Détection des anomalies')
    plt.xlabel('Cycle')
    plt.ylabel('Valeur')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return anomalies

# Détecter les anomalies
anomalies = detecter_anomalies(all_sensors_data, capteurs_a_analyser)

def analyser_relations_non_lineaires(all_data, sensor_list):

    from scipy import stats

    plt.figure(figsize=(15, 10))
    relations = {}

    for sensor in sensor_list:
        if sensor != 'SE':
            # Calculer la corrélation de Spearman
            correlation, p_value = stats.spearmanr(
                all_data.groupby('cycle_id')[sensor].mean(),
                all_data.groupby('cycle_id')['SE'].mean()
            )
            relations[sensor] = {'correlation': correlation, 'p_value': p_value}

            plt.scatter(
                all_data.groupby('cycle_id')[sensor].mean(),
                all_data.groupby('cycle_id')['SE'].mean(),
                alpha=0.5,
                label=f'{sensor} (ρ={correlation:.2f})'
            )

    plt.title('Relations non linéaires avec l\'efficacité du système')
    plt.xlabel('Valeur du capteur')
    plt.ylabel('Efficacité (SE)')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return relations

# Analyser les relations non linéaires
relations = analyser_relations_non_lineaires(all_sensors_data, capteurs_a_analyser)

def decomposer_serie_temporelle(sensor_name, cycle_id=0, freq=None):


    # Charger les données
    fichier = f'{long_format_path}/{sensor_name}_long.csv'
    df = pd.read_csv(fichier)

    # Filtrer pour le cycle spécifique et trier par temps
    cycle_data = df[df['cycle_id'] == cycle_id].sort_values('Temps (s)')

    # Ajuster la fréquence en fonction du capteur
    sampling_freqs = {
        'PS1': 100, 'PS2': 100, 'PS3': 100, 'PS5': 100, 'PS6': 100,
        'EPS1': 100,
        'FS1': 10, 'FS2': 10,
        'TS1': 1, 'TS2': 1, 'TS3': 1, 'TS4': 1,
        'VS1': 1,
        'CE': 1, 'SE': 1
    }

    # Définir la période pour la décomposition
    if freq is None:
        base_freq = sampling_freqs.get(sensor_name, 1)
        freq = max(2, min(len(cycle_data) // 3, base_freq * 2))

    # Créer une série temporelle
    ts = pd.Series(cycle_data['Valeur'].values,
                  index=pd.to_datetime(cycle_data['Temps (s)'], unit='s'))

    try:
        # Décomposition avec gestion des erreurs
        if len(ts) >= 2 * freq:  # Vérifier qu'il y a assez de points
            decomposition = seasonal_decompose(ts, period=freq, model='additive')

            # Visualisation
            plt.figure(figsize=(14, 12))

            plt.subplot(411)
            plt.plot(ts.index, ts.values)
            plt.title(f'Série originale - {sensor_name}, Cycle {cycle_id}')
            plt.grid(True)

            plt.subplot(412)
            plt.plot(decomposition.trend)
            plt.title('Tendance')
            plt.grid(True)

            plt.subplot(413)
            plt.plot(decomposition.seasonal)
            plt.title(f'Saisonnalité (période={freq})')
            plt.grid(True)

            plt.subplot(414)
            plt.plot(decomposition.resid)
            plt.title('Résidus')
            plt.grid(True)

            plt.tight_layout()
            plt.show()

            return decomposition
        else:
            print(f"Pas assez de points pour {sensor_name} (n={len(ts)}). "
                  f"Minimum requis: {2 * freq}")
            return None

    except Exception as e:
        print(f"Erreur lors de la décomposition de {sensor_name}: {str(e)}")
        print(f"Essayez avec une fréquence plus petite. Points disponibles: {len(ts)}")
        return None

# Tester la décomposition avec les nouvelles fréquences ajustées
for sensor in capteurs_a_analyser:
    print(f"\nAnalyse de {sensor}")
    # Utiliser une fréquence adaptée au capteur
    if sensor in ['PS1', 'PS2', 'PS3', 'PS5', 'PS6', 'EPS1']:
        freq = 200
    elif sensor in ['FS1', 'FS2']:
        freq = 20
    else:
        freq = 4

    decomposer_serie_temporelle(sensor, cycle_id=0, freq=freq)
    analyser_autocorrelation(sensor, cycle_id=0)

def prepare_features(sensor_list):


    features_df = pd.DataFrame()

    for sensor in sensor_list:
        # charger les sensors
        df = pd.read_csv(f'{long_format_path}/{sensor}_long.csv')

        # Calculate statistical features per cycle
        stats = df.groupby('cycle_id')['Valeur'].agg([
            'mean', 'std', 'min', 'max',
            lambda x: np.percentile(x, 25),
            lambda x: np.percentile(x, 75)
        ]).rename(columns={
            '<lambda_0>': f'{sensor}_25th',
            '<lambda_1>': f'{sensor}_75th'
        })

        # ajouter les noms et prefixe des sensors
        stats.columns = [f'{sensor}_{col}' for col in stats.columns]

        # ajoputer des features
        features_df = pd.concat([features_df, stats], axis=1)

    return features_df

for sensor in capteurs_a_analyser:
    decomposer_serie_temporelle(sensor, cycle_id=0, freq=200)
    analyser_autocorrelation(sensor, cycle_id=0)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def prepare_modeling_data():

    # charger features
    features_df = prepare_features(capteurs_a_analyser)

    # charger profile data
    profile = pd.read_csv(f'{output_path}/profile.csv')

    # Combine features with profile data
    X = pd.concat([features_df, profile], axis=1)

    # Target variable (SE mean)
    y = X['SE_mean']
    X = X.drop(['SE_mean'], axis=1)

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

def train_evaluate_model():
    """Train and evaluate prediction model"""
    # Prepare data
    X_train, X_test, y_train, y_test = prepare_modeling_data()

    # Train model
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # predictions
    y_pred = model.predict(X_test)

    # Evaluation
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"Mean Squared Error: {mse:.4f}")
    print(f"R² Score: {r2:.4f}")

    # Plot
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.xlabel('Actual Efficiency')
    plt.ylabel('Predicted Efficiency')
    plt.title('Predicted vs Actual System Efficiency')
    plt.tight_layout()
    plt.show()

    return model



from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf


def forecast_efficiency(sensor_name='SE', n_periods=10):

    df = pd.read_csv(f'{long_format_path}/{sensor_name}_long.csv')

    # Get mean efficiency per cycle
    ts = df.groupby('cycle_id')['Valeur'].mean()

    # Fit ARIMA model
    model = ARIMA(ts, order=(1, 1, 1))
    results = model.fit()

    # Résumé du modèle
    print(results.summary())

    # --- 1. Valeurs ajustées vs réelles
    plt.figure(figsize=(12, 5))
    plt.plot(ts, label='Valeurs réelles')
    plt.plot(results.fittedvalues, label='Valeurs ajustées (fitted)', color='orange')
    plt.title(f'Valeurs réelles vs ajustées - {sensor_name}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # --- 2. Résidus
    residuals = results.resid

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(residuals)
    plt.title("Résidus du modèle")

    plt.subplot(1, 2, 2)
    pd.Series(residuals).plot(kind='kde')
    plt.title("Distribution des résidus")
    plt.tight_layout()
    plt.show()

    # --- 3. Autocorrélation des résidus
    plot_acf(residuals, lags=30)
    plt.title("Autocorrélation des résidus")
    plt.show()

    # --- 4. Prévision avec intervalle de confiance
    forecast_result = results.get_forecast(steps=n_periods)
    forecast = forecast_result.predicted_mean
    conf_int = forecast_result.conf_int()

    plt.figure(figsize=(12, 6))
    plt.plot(ts.index, ts.values, label='Historique')
    plt.plot(
        range(len(ts), len(ts) + n_periods),
        forecast,
        'r--',
        label='Prévision'
    )
    plt.fill_between(
        range(len(ts), len(ts) + n_periods),
        conf_int.iloc[:, 0],
        conf_int.iloc[:, 1],
        color='pink',
        alpha=0.3,
        label='Intervalle de confiance'
    )
    plt.title(f'Prévision de l\'efficacité - {sensor_name}')
    plt.xlabel('Cycle')
    plt.ylabel('Valeur moyenne')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return forecast

if __name__ == "__main__":
    print(" Entraînement du modèle ML...")
    model = train_evaluate_model()

    print("\n Prévision ARIMA de l'efficacité future...")
    forecast_efficiency('SE', n_periods=100)

def prepare_data_for_models():

    se_data = pd.read_csv(f'{long_format_path}/SE_long.csv')
    se_values = se_data.groupby('cycle_id')['Valeur'].mean().values

    train_size = int(len(se_values) * 0.8)
    train, test = se_values[:train_size], se_values[train_size:]

    return train, test, se_values

def train_sarimax(train_data, test_data):
    # SARIMAX model
    model = SARIMAX(train_data,
                    order=(1, 1, 1),
                    seasonal_order=(1, 1, 1, 12))

    results = model.fit()

    #  predictions
    forecast = results.forecast(steps=len(test_data))

    #  metrics
    mse = mean_squared_error(test_data, forecast)
    r2 = r2_score(test_data, forecast)

    print(f"SARIMAX - MSE: {mse:.4f}, R²: {r2:.4f}")

    return results, forecast

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.graphics.tsaplots import plot_acf



def plot_sarimax_results(train, test, forecast, full_series):
    total_length = len(train) + len(test)
    x_train = np.arange(len(train))
    x_test = np.arange(len(train), total_length)

    plt.figure(figsize=(12, 6))
    plt.plot(np.arange(len(full_series)), full_series, label='Série réelle')
    plt.plot(x_test, forecast, 'r--', label='Prévision')
    plt.axvline(x=len(train), color='grey', linestyle='--', label='Split Train/Test')
    plt.title("Prévision SARIMAX sur l'efficacité moyenne (SE)")
    plt.xlabel("Cycle")
    plt.ylabel("Efficacité")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def plot_residuals(results):
    residuals = results.resid

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(residuals)
    plt.title("Résidus")

    plt.subplot(1, 2, 2)
    pd.Series(residuals).plot(kind='kde') #Kernel Density Estimation
    plt.title("Distribution des résidus")
    plt.tight_layout()
    plt.show()

    plot_acf(residuals, lags=30)
    plt.title("Autocorrélation des résidus")
    plt.tight_layout()
    plt.show()

train, test, full_series = prepare_data_for_models()
results, forecast = train_sarimax(train, test)
plot_sarimax_results(train, test, forecast, full_series)
plot_residuals(results)

def prepare_data_for_deep_learning(data, look_back=10):
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data.reshape(-1, 1))

    X, y = [], []
    for i in range(len(scaled_data) - look_back):
        X.append(scaled_data[i:(i + look_back), 0])
        y.append(scaled_data[i + look_back, 0])

    X = np.array(X)
    y = np.array(y)

    # Reshape pour deep learning [samples, time steps, features]
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    return X, y, scaler

def improved_sarimax(train_data, test_data):
    # Grid search pour les meilleurs paramètres
    best_aic = float('inf')
    best_params = None

    p_values = range(0, 3)
    d_values = range(0, 2)
    q_values = range(0, 3)

    for p in p_values:
        for d in d_values:
            for q in q_values:
                try:
                    model = SARIMAX(train_data,
                                  order=(p, d, q),
                                  seasonal_order=(1, 1, 1, 12))
                    results = model.fit(disp=False)
                    aic = results.aic

                    if aic < best_aic:
                        best_aic = aic
                        best_params = (p, d, q)
                except:
                    continue

    print(f"Meilleurs paramètres SARIMAX: {best_params}")

    # Entraîner le modèle avec les meilleurs paramètres
    final_model = SARIMAX(train_data,
                         order=best_params,
                         seasonal_order=(1, 1, 1, 12))
    final_results = final_model.fit(disp=False)

    forecast = final_results.forecast(steps=len(test_data))

    mse = mean_squared_error(test_data, forecast)
    r2 = r2_score(test_data, forecast)

    print(f"SARIMAX amélioré - MSE: {mse:.4f}, R²: {r2:.4f}")

    return final_results, forecast

# Charger les données
train, test, full_series = prepare_data_for_models()

# Exécuter le modèle SARIMAX
sarimax_results, sarimax_forecast = improved_sarimax(train, test)

def plot_sarimax_forecast(test, sarimax_forecast, look_back):
    plt.figure(figsize=(15, 8))
    plt.plot(test, label='Réel', color='black')
    plt.plot(range(len(test)), sarimax_forecast, label='Prévisions SARIMAX', linestyle='--', color='blue')
    plt.title('Comparaison des prévisions SARIMAX avec les valeurs réelles')
    plt.xlabel('Temps')
    plt.ylabel('Valeur')
    plt.legend()
    plt.grid(True)
    plt.show()

# Définir look_back dans le contexte global
look_back = 10  # Exemple de valeur de look_back

# Appeler la fonction avec les données nécessaires
plot_sarimax_forecast(test, sarimax_forecast, look_back)

class LSTMModel:
    def __init__(self, input_shape, units, learning_rate=0.001):
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import LSTM, Dense, Dropout

        self.model = Sequential()
        self.model.add(LSTM(units, input_shape=input_shape, return_sequences=True))
        self.model.add(Dropout(0.2))
        self.model.add(LSTM(units))
        self.model.add(Dropout(0.2))
        self.model.add(Dense(1))  # Assuming a regression task

        self.model.compile(optimizer='adam', loss='mean_squared_error')

    def train(self, X_train, y_train, epochs=50, batch_size=32, validation_data=None):
        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)

    def evaluate(self, X_test, y_test):
        return self.model.evaluate(X_test, y_test)

    def predict(self, X):
        return self.model.predict(X)

class GRUModel:
    def __init__(self, input_shape, units):
        self.model = self.build_model(input_shape, units)

    def build_model(self, input_shape, units):
        model = models.Sequential()
        model.add(layers.GRU(units, activation='relu', input_shape=input_shape))
        model.add(layers.Dense(1))  # Assuming a regression task
        model.compile(optimizer='adam', loss='mean_squared_error')
        return model

    def train(self, X_train, y_train, epochs=50, batch_size=32, validation_data=None):
        history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)
        return history

    def evaluate(self, X_test, y_test):
        loss = self.model.evaluate(X_test, y_test)
        return loss

    def predict(self, X):
        predictions = self.model.predict(X)
        return predictions

class ANNModel:
    def __init__(self, input_shape, output_units):
        self.model = Sequential()
        self.model.add(Dense(64, activation='relu', input_shape=input_shape))
        self.model.add(Dropout(0.2))
        self.model.add(Dense(32, activation='relu'))
        self.model.add(Dropout(0.2))
        self.model.add(Dense(output_units, activation='linear'))

    def compile(self, optimizer='adam', loss='mean_squared_error'):
        self.model.compile(optimizer=optimizer, loss=loss)

    def train(self, X_train, y_train, epochs=100, batch_size=32, validation_data=None):
        history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)
        return history

    def evaluate(self, X_test, y_test):
        loss = self.model.evaluate(X_test, y_test)
        return loss

    def predict(self, X):
        predictions = self.model.predict(X)
        return predictions

def normalize_data(data):
    """Normalize the dataset to a range of [0, 1]."""
    return (data - data.min()) / (data.max() - data.min())

def handle_missing_values(data, method='mean'):
    """Handle missing values in the dataset."""
    if method == 'mean':
        return data.fillna(data.mean())
    elif method == 'median':
        return data.fillna(data.median())
    elif method == 'drop':
        return data.dropna()
    else:
        raise ValueError("Method must be 'mean', 'median', or 'drop'.")

def split_dataset(data, train_size=0.8):
    """Split the dataset into training and testing sets."""
    train_size = int(len(data) * train_size)
    train, test = data[:train_size], data[train_size:]
    return train, test

import pandas as pd

def load_csv(file_path):
    """Load a CSV file into a pandas DataFrame."""
    try:
        data = pd.read_csv(file_path)
        return data
    except Exception as e:
        print(f"Error loading CSV file: {e}")
        return None

def load_database(connection_string, query):
    """Load data from a database using a SQL query."""
    import sqlalchemy

    try:
        engine = sqlalchemy.create_engine(connection_string)
        data = pd.read_sql(query, engine)
        return data
    except Exception as e:
        print(f"Error loading data from database: {e}")
        return None

def load_data(file_path, source_type='csv', connection_string=None, query=None):
    """Load data from specified source type."""
    if source_type == 'csv':
        return load_csv(file_path)
    elif source_type == 'database':
        return load_database(connection_string, query)
    else:
        print("Unsupported source type. Please use 'csv' or 'database'.")
        return None

import matplotlib.pyplot as plt

def plot_time_series(data, title='Time Series Data', xlabel='Time', ylabel='Value'):
    plt.figure(figsize=(12, 6))
    plt.plot(data, color='blue')
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.grid(True)
    plt.show()

def compare_models(actual, predictions, model_names):
    plt.figure(figsize=(14, 7))
    plt.plot(actual, label='Actual', color='black', linewidth=2)

    for prediction, model_name in zip(predictions, model_names):
        plt.plot(prediction, label=model_name)

    plt.title('Model Predictions vs Actual')
    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    plt.show()

def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    # Check if validation loss exists before plotting
    if 'val_loss' in history.history:
        plt.plot(history.history['val_loss'], label='Validation Loss')
    else:
        print("Warning: Validation loss not found in history. Plotting only training loss.")
    plt.title('Model Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np

def mean_absolute_error(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

def r_squared(y_true, y_pred):
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    return 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout
from tensorflow.keras.optimizers import Adam

train, test, full_series = prepare_data_for_models()
results, forecast = improved_sarimax(train, test)
plot_sarimax_results(train, test, forecast, full_series)
plot_residuals(results)

# Prepare data for deep learning models
look_back = 100
X, y, scaler = prepare_data_for_deep_learning(full_series, look_back)

# Split into train and test sets
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Train LSTM model
lstm_model = LSTMModel(input_shape=(look_back, 1), units=100)
lstm_model.train(X_train, y_train, epochs=100)

lstm_predictions = lstm_model.predict(X_test)

import matplotlib.pyplot as plt
import numpy as np
# Make predictions with LSTM
lstm_predictions = lstm_model.predict(X_test)
lstm_predictions = scaler.inverse_transform(lstm_predictions)

# Reshape predictions to 1D if they are 2D
if lstm_predictions.ndim == 2 and lstm_predictions.shape[1] == 1:
    lstm_predictions = lstm_predictions.flatten()


# Plotting the LSTM forecast
def plot_lstm_forecast(train_size, lstm_predictions, full_series, look_back):
    # Calculate the index in the original full_series where the first prediction corresponds.
    # The first prediction corresponds to the first element in y_test,
    # which is y[train_size]. This y corresponds to full_series[look_back + train_size].
    prediction_start_index = look_back + train_size

    # The x-axis values for the predictions and corresponding actual test data
    # It should start from the index in full_series that the first prediction targets
    # and have the same length as the number of predictions.
    x_axis_predictions = np.arange(prediction_start_index, prediction_start_index + len(lstm_predictions))

    # The actual data points that the LSTM is predicting are from full_series starting
    # at prediction_start_index and having the same length as lstm_predictions.
    actual_test_for_comparison = full_series[prediction_start_index : prediction_start_index + len(lstm_predictions)]

    # Ensure the shapes match before plotting the test data
    if actual_test_for_comparison.shape[0] != x_axis_predictions.shape[0]:
         raise ValueError(f"Shape mismatch: actual_test_for_comparison has shape {actual_test_for_comparison.shape} "
                         f"but x_axis_predictions has shape {x_axis_predictions.shape}")


    plt.figure(figsize=(15, 8))

    # Plot the historical data (up to the point where predictions start)
    # Plotting up to prediction_start_index (exclusive)
    plt.plot(np.arange(prediction_start_index), full_series[:prediction_start_index], label='Historical Data')

    # Plot the actual test data that aligns with the predictions
    plt.plot(x_axis_predictions, actual_test_for_comparison, label='Actual Test Data', color='black')

    # Plot the LSTM predictions
    plt.plot(x_axis_predictions, lstm_predictions, label='LSTM Predictions', linestyle='--', color='red')

    plt.title('LSTM Forecast vs Actual Data')
    plt.xlabel('Cycle Index')
    plt.ylabel('System Efficiency (SE)')
    plt.legend()
    plt.grid(True)
    plt.show()


# Now call the function to plot with the necessary arguments
# Pass train_size, lstm_predictions, full_series, and look_back
plot_lstm_forecast(train_size, lstm_predictions, full_series, look_back)

import os
from tensorflow.keras.models import load_model

# Define the path where you want to save/load the model
model_save_path = '/content/drive/MyDrive/Projet_time_series/lstm_model.h5'

# Save the trained LSTM model
print(f"Saving the trained LSTM model to: {model_save_path}")
lstm_model.model.save(model_save_path)
print("LSTM model saved successfully.")

# Train GRU model
gru_model = GRUModel(input_shape=(look_back, 1), units=50)
gru_history = gru_model.train(X_train, y_train, epochs=50)

# Train ANN model
ann_model = ANNModel(input_shape=(look_back, 1), output_units=1)
ann_model.compile()
ann_history = ann_model.train(X_train, y_train, epochs=100)

# Make predictions
lstm_predictions = lstm_model.predict(X_test)
gru_predictions = gru_model.predict(X_test)
ann_predictions = ann_model.predict(X_test)

# Inverse transform the predictions to original scale
lstm_predictions = scaler.inverse_transform(lstm_predictions)
gru_predictions = scaler.inverse_transform(gru_predictions)

# Reshape ANN predictions to (n_samples, n_features) before inverse scaling
# Assuming the prediction for each sample is the last value in the sequence
ann_predictions_reshaped = ann_predictions[:, -1, :] # Select the last time step
ann_predictions = scaler.inverse_transform(ann_predictions_reshaped)

y_test = scaler.inverse_transform(y_test.reshape(-1, 1))

# Evaluate Models
# Note: Evaluation metrics usually compare actual vs predicted on the original scale
# You might need to adjust the evaluate method if it expects scaled data
lstm_loss = mean_squared_error(y_test, lstm_predictions) # Using custom MSE
gru_loss = mean_squared_error(y_test, gru_predictions)   # Using custom MSE
ann_loss = mean_squared_error(y_test, ann_predictions)   # Using custom MSE

print(f"LSTM Loss (MSE): {lstm_loss}")
print(f"GRU Loss (MSE): {gru_loss}")
print(f"ANN Loss (MSE): {ann_loss}")

# Compare the models
print("gru loss")
plot_loss(gru_history)
print("ann loss")
plot_loss(ann_history)

# Pour les modèles de Deep Learning, les prédictions correspondent à X_test
# L'index temporel pour les prédictions doit correspondre à la fin de chaque fenêtre de look_back
# dans la période de test.
# L'index de y_test_original correspond aux points prédits.
# test_index_for_dl_preds = full_series.index[len(X_train) + look_back : len(X_train) + look_back + len(y_test_original)]

# Generate indices for the test set predictions since full_series is a numpy array
start_index = len(X_train) + look_back
end_index = start_index + len(y_test_original)
test_index_for_dl_preds = np.arange(start_index, end_index)


# Plot LSTM predictions
plt.figure(figsize=(10, 6))
# Use np.arange for the full series index as well, as it's a numpy array
plt.plot(np.arange(len(full_series)), full_series, label='Actual Full Series', color='gray', alpha=0.7)
plt.plot(test_index_for_dl_preds, y_test_original, label='Actual Test', color='black', linewidth=2)
plt.plot(test_index_for_dl_preds, lstm_predictions, label='LSTM Predictions', color='red', linestyle='--')
plt.title('LSTM Model Predictions vs Actual')
plt.xlabel('Index du cycle')
plt.ylabel('Valeur')
plt.legend()
plt.grid(True)
plt.show()

# Plot GRU predictions
plt.figure(figsize=(10, 6))
# Use np.arange for the full series index as well
plt.plot(np.arange(len(full_series)), full_series, label='Actual Full Series', color='gray', alpha=0.7)
plt.plot(test_index_for_dl_preds, y_test_original, label='Actual Test', color='black', linewidth=2)
plt.plot(test_index_for_dl_preds, gru_predictions, label='GRU Predictions', color='green', linestyle='--')
plt.title('GRU Model Predictions vs Actual')
plt.xlabel('Index du cycle')
plt.ylabel('Valeur')
plt.legend()
plt.grid(True)
plt.show()

# Plot ANN predictions
plt.figure(figsize=(10, 6))
# Use np.arange for the full series index as well
plt.plot(np.arange(len(full_series)), full_series, label='Actual Full Series', color='gray', alpha=0.7)
plt.plot(test_index_for_dl_preds, y_test_original, label='Actual Test', color='black', linewidth=2)
plt.plot(test_index_for_dl_preds, ann_predictions, label='ANN Predictions', color='purple', linestyle='--')
plt.title('ANN Model Predictions vs Actual')
plt.xlabel('Index du cycle')
plt.ylabel('Valeur')
plt.legend()
plt.grid(True)
plt.show()


# Optionnellement, vous pouvez utiliser la fonction compare_models si elle convient à ce format
# Note: La fonction compare_models actuelle prend une liste de prédictions et un seul ensemble 'actual'.
# Ici, 'actual' serait y_test_original, et les prédictions sont les sorties dés-encapsulées.
# compare_models(y_test_original, [lstm_predictions, gru_predictions, ann_predictions],
#                 ['LSTM', 'GRU', 'ANN'])